{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efadbfe",
   "metadata": {},
   "source": [
    "\n",
    "# Rocket â€” Unified Matching (STRICT Local Gating Fix)\n",
    "\n",
    "**What's new in this fix**\n",
    "- **Strict candidate filtering** in *local* mode: before ranking, we **compute allowed IDs** using `(distance <= radius)` **and** shared language (if required). We then rank **only** those IDs.  \n",
    "- This prevents remote users with `0.0` scores from being selected to pad results.\n",
    "- `mmr_rank`/`dpp_greedy` now accept an `allowed` list.\n",
    "\n",
    "See citations inline (MMR, YouTube's two-stage architecture, haversine/geodesic distance) for rationale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca0ba4",
   "metadata": {},
   "source": [
    "## Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dedbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, random, math, importlib, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import date, datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "import networkx as nx\n",
    "\n",
    "np.random.seed(777); random.seed(777)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a88d5",
   "metadata": {},
   "source": [
    "## Embeddings & utilities (same as unified notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64407f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_names: List[str] = None):\n",
    "        self.model = None\n",
    "        self.sbert_ok = False\n",
    "        self.tfidf = None\n",
    "        self.model_names = model_names or [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        ]\n",
    "        if importlib.util.find_spec(\"sentence_transformers\") is not None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            for name in self.model_names:\n",
    "                try:\n",
    "                    self.model = SentenceTransformer(name)\n",
    "                    self.sbert_ok = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        if self.sbert_ok:\n",
    "            return self\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "        self.tfidf.fit(corpus)\n",
    "        return self\n",
    "\n",
    "    def encode(self, items: List[str]) -> np.ndarray:\n",
    "        if self.sbert_ok:\n",
    "            return np.array(self.model.encode(items, show_progress_bar=False, normalize_embeddings=True))\n",
    "        X = self.tfidf.transform(items)\n",
    "        X = X.astype(np.float64)\n",
    "        norms = np.sqrt((X.power(2)).sum(axis=1))\n",
    "        norms[norms==0] = 1.0\n",
    "        return (X / norms).toarray()\n",
    "\n",
    "def parse_dob(dob_str: str) -> date:\n",
    "    return datetime.strptime(dob_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "def compute_age(dob: date, today: Optional[date] = None) -> int:\n",
    "    today = today or date.today()\n",
    "    years = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "    return max(0, years)\n",
    "\n",
    "def age_band(age: int) -> str:\n",
    "    for lo, hi in [(18,24),(25,34),(35,44),(45,54)]:\n",
    "        if lo <= age <= hi: return f\"{lo}-{hi}\"\n",
    "    return \"55+\"\n",
    "\n",
    "@dataclass\n",
    "class BigFive:\n",
    "    O: float; C: float; E: float; A: float; N: float\n",
    "\n",
    "def clip01(x): \n",
    "    import numpy as np\n",
    "    return float(np.clip(x, 0.0, 1.0))\n",
    "\n",
    "TIPI_KEY = {\n",
    "    1: (\"E\", False), 2: (\"A\", True), 3: (\"C\", False), 4: (\"N\", False), 5: (\"O\", False),\n",
    "    6: (\"E\", True),  7: (\"A\", False),8: (\"C\", True),  9: (\"N\", True),  10:(\"O\", True)\n",
    "}\n",
    "\n",
    "def score_tipi(responses_1to7):\n",
    "    import numpy as np\n",
    "    assert len(responses_1to7)==10\n",
    "    r = np.array(responses_1to7, dtype=float)\n",
    "    r01 = (r-1)/6.0\n",
    "    traits = {\"O\":[], \"C\":[], \"E\":[], \"A\":[], \"N\":[]}\n",
    "    for i,val in enumerate(r01, start=1):\n",
    "        trait, rev = TIPI_KEY[i]\n",
    "        traits[trait].append(1.0-val if rev else val)\n",
    "    return BigFive(*(clip01(np.mean(traits[t])) for t in [\"O\",\"C\",\"E\",\"A\",\"N\"]))\n",
    "\n",
    "def bigfive_cosine(u: BigFive, v: BigFive) -> float:\n",
    "    import numpy as np\n",
    "    a = np.array([u.O,u.C,u.E,u.A,u.N])\n",
    "    b = np.array([v.O,v.C,v.E,v.A,v.N])\n",
    "    return float(a @ b / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498063b4",
   "metadata": {},
   "source": [
    "## Intake + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3eac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_intake(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    def wclip(t): \n",
    "        ws = (t or \"\").split()\n",
    "        return \" \".join(ws[:250])\n",
    "    dob_str = row.get(\"dob\",\"1989-01-01\")\n",
    "    try:\n",
    "        dob = parse_dob(dob_str)\n",
    "    except Exception:\n",
    "        dob = date(1989,1,1); dob_str=\"1989-01-01\"\n",
    "    age_val = compute_age(dob)\n",
    "    return {\n",
    "        \"name\": row.get(\"name\",\"Unnamed\"),\n",
    "        \"dob\": dob_str, \"age\": age_val, \"age_band\": age_band(age_val),\n",
    "        \"location_city\": row.get(\"location_city\",\"\"), \"location_country\": row.get(\"location_country\",\"\"),\n",
    "        \"lat\": float(row.get(\"lat\", 43.6532)), \"lon\": float(row.get(\"lon\", -79.3832)),\n",
    "        \"tz_offset\": int(row.get(\"tz_offset\", -5)),\n",
    "        \"languages\": row.get(\"languages\",\"en\"),\n",
    "        \"availability_hours\": row.get(\"availability_hours\",\"5-10\"),\n",
    "        \"energy_1to5\": int(row.get(\"energy_1to5\",3)),\n",
    "        \"collab_style\": row.get(\"collab_style\",\"hybrid\"),\n",
    "        \"role\": row.get(\"role\",\"Undecided\"),\n",
    "        \"seniority\": row.get(\"seniority\",\"Mid\"),\n",
    "        \"years_exp\": int(row.get(\"years_exp\",3)),\n",
    "        \"skills_have\": row.get(\"skills_have\",\"\"),\n",
    "        \"skills_want\": row.get(\"skills_want\",row.get(\"interests\",\"\")),\n",
    "        \"interests\": row.get(\"interests\",\"\"),\n",
    "        \"human\": wclip(row.get(\"human\",\"\")),\n",
    "        \"professional\": wclip(row.get(\"professional\",\"\")),\n",
    "        \"contributor\": wclip(row.get(\"contributor\",\"\")),\n",
    "        \"interests_long\": wclip(row.get(\"interests_long\",\"\")),\n",
    "        \"reason\": wclip(row.get(\"reason\",\"\")),\n",
    "    }\n",
    "\n",
    "def parse_list_csv(s: str) -> List[str]:\n",
    "    return [x.strip() for x in (s or \"\").split(\",\") if x.strip()]\n",
    "\n",
    "def build_text_similarity(df, embedder):\n",
    "    corpus = (df['interests'].fillna('') + \" ; \" + df['skills_have'].fillna('') + \" ; \" + df['professional'].fillna('')).tolist()\n",
    "    embedder.fit(corpus)\n",
    "    X = embedder.encode(corpus)\n",
    "    S = (X @ X.T)\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    return S\n",
    "\n",
    "def language_overlap(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    langs = [set([x.strip().lower() for x in (l or '').split(',') if x.strip()]) for l in df['languages'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            inter = langs[i] & langs[j]\n",
    "            uni = langs[i] | langs[j]\n",
    "            S[i,j] = len(inter)/float(len(uni) + 1e-9)\n",
    "    return S\n",
    "\n",
    "def geo_distance_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); D=np.zeros((n,n))\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            D[i,j] = geodesic(coords[i], coords[j]).km\n",
    "    return D\n",
    "\n",
    "def geo_similarity_from_D(D: np.ndarray, decay_km: float = 1200.0) -> np.ndarray:\n",
    "    S = np.exp(-D/decay_km)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def experience_compatibility(years: List[int], sweet_spot: float = 3.0) -> np.ndarray:\n",
    "    years = np.array(years); n=len(years); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(years[i]-years[j])\n",
    "            S[i,j] = np.exp(-((gap-sweet_spot)**2)/(2*(sweet_spot**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "ROLE_COMP = {\n",
    "    \"Founder\": {\"Engineer\": 1.0, \"Designer\": 1.0, \"Researcher\": 0.8, \"Founder\": 0.2, \"Writer\":0.6, \"Scientist\":0.7, \"Creator\":0.8},\n",
    "    \"Engineer\": {\"Founder\": 1.0, \"Designer\": 0.7, \"Engineer\": 0.2, \"Researcher\": 0.6, \"Writer\":0.6, \"Scientist\":0.8, \"Creator\":0.7},\n",
    "    \"Designer\": {\"Founder\": 1.0, \"Engineer\": 0.7, \"Designer\": 0.2, \"Researcher\": 0.5, \"Writer\":0.6, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Researcher\": {\"Founder\": 0.8, \"Engineer\": 0.7, \"Designer\": 0.5, \"Researcher\": 0.3, \"Writer\":0.5, \"Scientist\":0.9, \"Creator\":0.6},\n",
    "    \"Writer\": {\"Founder\":0.8, \"Engineer\":0.6, \"Designer\":0.7, \"Researcher\":0.5, \"Writer\":0.2, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Scientist\":{\"Founder\":0.9, \"Engineer\":0.9, \"Designer\":0.5, \"Researcher\":0.8, \"Writer\":0.5, \"Scientist\":0.2, \"Creator\":0.6},\n",
    "    \"Creator\":{\"Founder\":0.9, \"Engineer\":0.7, \"Designer\":0.9, \"Researcher\":0.6, \"Writer\":0.9, \"Scientist\":0.6, \"Creator\":0.3},\n",
    "    \"Undecided\": {\"Founder\":0.6,\"Engineer\":0.6,\"Designer\":0.6,\"Researcher\":0.6,\"Writer\":0.6,\"Scientist\":0.6,\"Creator\":0.6,\"Undecided\":0.2}\n",
    "}\n",
    "\n",
    "def role_complementarity(df: pd.DataFrame) -> np.ndarray:\n",
    "    roles = df['role'].tolist(); n=len(roles); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = ROLE_COMP.get(roles[i], {}).get(roles[j], 0.2)\n",
    "    return S\n",
    "\n",
    "def energy_compatibility(energies: List[int], target_gap=0):\n",
    "    e = np.array(energies); n=len(e); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(e[i]-e[j])\n",
    "            S[i,j] = np.exp(-((gap-target_gap)**2)/(2*(1.25**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "COLLAB_COMP = {\n",
    "    \"async\": {\"async\":1.0, \"hybrid\":0.7, \"sync\":0.3},\n",
    "    \"hybrid\":{\"async\":0.7, \"hybrid\":1.0, \"sync\":0.7},\n",
    "    \"sync\":  {\"async\":0.3, \"hybrid\":0.7, \"sync\":1.0},\n",
    "}\n",
    "def collab_style_compatibility(styles: List[str]) -> np.ndarray:\n",
    "    n=len(styles); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = COLLAB_COMP.get(styles[i],{}).get(styles[j], 0.5)\n",
    "    return S\n",
    "\n",
    "def availability_overlap(avails: List[str]) -> np.ndarray:\n",
    "    map_mid = {\"2-5\":3.5,\"5-10\":7.5,\"10-20\":15.0,\"20+\":25.0}\n",
    "    v = np.array([map_mid.get(a,7.5) for a in avails])\n",
    "    n=len(v); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(v[i]-v[j])\n",
    "            S[i,j] = np.exp(-gap/15.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def time_zone_overlap(tz_list: List[int]) -> np.ndarray:\n",
    "    tz = np.array(tz_list); n=len(tz); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            diff = abs(tz[i]-tz[j])\n",
    "            S[i,j] = np.exp(-diff/6.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613519a0",
   "metadata": {},
   "source": [
    "## Skills & CF/Graph/Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a8aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_skill_list(sk: str) -> List[str]:\n",
    "    return [s.strip().lower() for s in (sk or \"\").split(\",\") if s.strip()]\n",
    "\n",
    "def tfidf_cosine(a_list: List[str], b_list: List[str]) -> float:\n",
    "    docs = [\"; \".join(a_list), \"; \".join(b_list)]\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "    X = vec.fit_transform(docs)\n",
    "    return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "\n",
    "def similar_skills_matrix(df: pd.DataFrame, embedder: Optional[Embedder] = None) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    parsed = [parse_skill_list(x) for x in df['skills_have'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = tfidf_cosine(parsed[i], parsed[j])\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def complementary_skills_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    wants = [parse_skill_list(row.get('skills_want', row.get('interests',''))) for _,row in df.iterrows()]\n",
    "    haves = [parse_skill_list(row.get('skills_have','')) for _,row in df.iterrows()]\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    try:\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                A = [\"; \".join([n1]) for n1 in need]\n",
    "                B = [\"; \".join([h1]) for h1 in have]\n",
    "                vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "                X = vec.fit_transform(A + B)\n",
    "                m, k = len(need), len(have)\n",
    "                Csim = np.zeros((m,k))\n",
    "                for p in range(m):\n",
    "                    for q in range(k):\n",
    "                        Csim[p,q] = cosine_similarity(X[p], X[m+q])[0,0]\n",
    "                size = max(m,k)\n",
    "                padded = np.ones((size,size))\n",
    "                padded[:m,:k] = 1.0 - Csim  # cost = 1 - sim\n",
    "                r_ind, c_ind = linear_sum_assignment(padded)\n",
    "                total_sim = 0.0; count = 0\n",
    "                for r,c in zip(r_ind, c_ind):\n",
    "                    if r < m and c < k:\n",
    "                        total_sim += 1.0 - padded[r,c]; count += 1\n",
    "                S[i,j] = total_sim / (count + 1e-9)\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    except Exception:\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                sims = []\n",
    "                for nterm in need:\n",
    "                    sims.append(max(tfidf_cosine([nterm], [h]) for h in have))\n",
    "                S[i,j] = float(np.mean(sims)) if sims else 0.0\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def mf_als(R: np.ndarray, k: int = 16, alpha: float = 40.0, reg: float = 0.1, iters: int = 6):\n",
    "    n_users, n_items = R.shape\n",
    "    C = 1 + alpha * R  # confidence\n",
    "    X = np.random.normal(scale=0.1, size=(n_users, k))\n",
    "    Y = np.random.normal(scale=0.1, size=(n_items, k))\n",
    "    eye = np.eye(k)\n",
    "    for _ in range(iters):\n",
    "        for u in range(n_users):\n",
    "            Cu = np.diag(C[u])\n",
    "            YTCuY = Y.T @ Cu @ Y\n",
    "            YTCuPu = Y.T @ (Cu @ R[u])\n",
    "            X[u] = np.linalg.solve(YTCuY + reg*eye, YTCuPu)\n",
    "        for i in range(n_items):\n",
    "            Ci = np.diag(C[:,i])\n",
    "            XTCiX = X.T @ Ci @ X\n",
    "            XTCiPi = X.T @ (Ci @ R[:,i])\n",
    "            Y[i] = np.linalg.solve(XTCiX + reg*eye, XTCiPi)\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    S = Xn @ Xn.T\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def build_graph_ppr(R: np.ndarray, alpha=0.85) -> np.ndarray:\n",
    "    n = R.shape[0]\n",
    "    G = nx.DiGraph(); G.add_nodes_from(range(n))\n",
    "    edges = [(u,v) for u in range(n) for v in range(n) if R[u,v]>0]\n",
    "    G.add_edges_from(edges)\n",
    "    S = np.zeros((n,n))\n",
    "    for u in range(n):\n",
    "        pr = nx.pagerank(G, alpha=alpha, personalization={k:(1.0 if k==u else 0.0) for k in range(n)})\n",
    "        for v,s in pr.items(): S[u,v] = s\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-12)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def reciprocalize(S: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(S * S.T + 1e-12)\n",
    "\n",
    "def combine_content(S_text, S_geo, S_exp, S_role, S_energy, S_collab, S_avail, S_tz, S_lang, w):\n",
    "    a,b,c,d,e,f,g,h,l = w\n",
    "    S = a*S_text + b*S_geo + c*S_exp + d*S_role + e*S_energy + f*S_collab + g*S_avail + h*S_tz + l*S_lang\n",
    "    return S / (S.max() + 1e-9)\n",
    "\n",
    "def fuse_scores(S_content, S_cf, S_graph, S_person, S_skills, weights=(0.30,0.18,0.16,0.10,0.26)):\n",
    "    Sc = reciprocalize(S_content)\n",
    "    Sf = reciprocalize(S_cf)\n",
    "    Sg = reciprocalize(S_graph)\n",
    "    Sp = reciprocalize(S_person)\n",
    "    Ss = reciprocalize(S_skills)\n",
    "    a,b,c,d,e = weights\n",
    "    S = a*Sc + b*Sf + c*Sg + d*Sp + e*Ss\n",
    "    return S / (S.max() + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595f794",
   "metadata": {},
   "source": [
    "## Diversification (now with `allowed` candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed87aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mmr_rank(query_idx: int, S: np.ndarray, K: int = 5, lambda_rel: float = 0.7, allowed: Optional[List[int]] = None):\n",
    "    n = S.shape[0]\n",
    "    base_candidates = [i for i in range(n) if i != query_idx]\n",
    "    candidates = [c for c in (allowed if allowed is not None else base_candidates) if c != query_idx]\n",
    "    selected = []\n",
    "    while candidates and len(selected) < K:\n",
    "        if not selected:\n",
    "            i = max(candidates, key=lambda j: S[query_idx, j])\n",
    "            selected.append(i); candidates.remove(i)\n",
    "        else:\n",
    "            def score(j):\n",
    "                redundancy = max(S[j, s] for s in selected) if selected else 0.0\n",
    "                return lambda_rel * S[query_idx, j] - (1-lambda_rel) * redundancy\n",
    "            i = max(candidates, key=score)\n",
    "            selected.append(i); candidates.remove(i)\n",
    "    return selected\n",
    "\n",
    "def dpp_greedy(query_idx: int, S: np.ndarray, K: int = 5, allowed: Optional[List[int]] = None):\n",
    "    n = S.shape[0]\n",
    "    items = [i for i in range(n) if i != query_idx]\n",
    "    items = [i for i in (allowed if allowed is not None else items) if i != query_idx]\n",
    "    quality = S[query_idx].copy()\n",
    "    q = quality / (quality.max() + 1e-9)\n",
    "    selected = []\n",
    "    remaining = items.copy()\n",
    "    while remaining and len(selected) < K:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax([q[i] for i in remaining]))\n",
    "            chosen = remaining[idx]\n",
    "        else:\n",
    "            scores = []\n",
    "            for r in remaining:\n",
    "                max_sim = max(S[r, s] for s in selected) if selected else 0.0\n",
    "                scores.append(q[r] - max_sim)\n",
    "            idx = int(np.argmax(scores))\n",
    "            chosen = remaining[idx]\n",
    "        selected.append(chosen); remaining.pop(idx)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eebb41",
   "metadata": {},
   "source": [
    "## Strict local gating + APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e576e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_signals(df: pd.DataFrame, embedder: Embedder):\n",
    "    D = geo_distance_matrix(df)\n",
    "    S_geo = geo_similarity_from_D(D, decay_km=1200.0)\n",
    "    S_text = build_text_similarity(df, embedder)\n",
    "    S_lang = language_overlap(df)\n",
    "    S_exp  = experience_compatibility(df['years_exp'].tolist())\n",
    "    S_role = role_complementarity(df)\n",
    "    S_energy = energy_compatibility(df['energy_1to5'].tolist())\n",
    "    S_collab = collab_style_compatibility(df['collab_style'].tolist())\n",
    "    S_avail  = availability_overlap(df['availability_hours'].tolist())\n",
    "    S_tz     = time_zone_overlap(df['tz_offset'].tolist())\n",
    "    S_sk_sim = similar_skills_matrix(df)\n",
    "    S_sk_comp = complementary_skills_matrix(df)\n",
    "    # Synthetic implicit likes -> CF + graph\n",
    "    n = len(df); R = np.zeros((n,n), dtype=float)\n",
    "    for _ in range(900):\n",
    "        u = random.randrange(n); v = random.randrange(n)\n",
    "        if u==v: continue\n",
    "        if df.iloc[u].role==\"Founder\" and df.iloc[v].role in [\"Engineer\",\"Designer\"]: R[u,v]=1.0\n",
    "        elif df.iloc[u].role==\"Creator\" and df.iloc[v].role in [\"Writer\",\"Designer\",\"Engineer\"]: R[u,v]=1.0\n",
    "        elif df.iloc[u].location_city==df.iloc[v].location_city and random.random()<0.25: R[u,v]=1.0\n",
    "        elif random.random() < 0.04: R[u,v]=1.0\n",
    "    S_cf = mf_als(R, k=16)\n",
    "    S_graph = build_graph_ppr(R, alpha=0.82)\n",
    "    # Personality\n",
    "    S_person = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S_person[i,j] = bigfive_cosine(df.iloc[i].bf, df.iloc[j].bf)\n",
    "    S_person = (S_person - S_person.min())/(S_person.max()-S_person.min()+1e-9)\n",
    "    return {\"D\":D,\"S_text\":S_text,\"S_lang\":S_lang,\"S_geo\":S_geo,\"S_exp\":S_exp,\"S_role\":S_role,\n",
    "            \"S_energy\":S_energy,\"S_collab\":S_collab,\"S_avail\":S_avail,\"S_tz\":S_tz,\n",
    "            \"S_sk_sim\":S_sk_sim,\"S_sk_comp\":S_sk_comp,\"S_cf\":S_cf,\"S_graph\":S_graph,\"S_person\":S_person}\n",
    "\n",
    "def fused_matrix(signals: Dict[str, np.ndarray],\n",
    "                 content_weights=(0.24,0.16,0.10,0.10,0.10,0.10,0.10,0.05,0.05),\n",
    "                 blend_weights=(0.34,0.18,0.14,0.12,0.22),\n",
    "                 skills_mode=\"similar\"):\n",
    "    S_content = combine_content(signals[\"S_text\"], signals[\"S_geo\"], signals[\"S_exp\"], signals[\"S_role\"],\n",
    "                                signals[\"S_energy\"], signals[\"S_collab\"], signals[\"S_avail\"], signals[\"S_tz\"],\n",
    "                                signals[\"S_lang\"], content_weights)\n",
    "    S_sk = signals[\"S_sk_sim\"] if skills_mode==\"similar\" else signals[\"S_sk_comp\"]\n",
    "    S_final = fuse_scores(S_content, signals[\"S_cf\"], signals[\"S_graph\"], signals[\"S_person\"], S_sk, weights=blend_weights)\n",
    "    return S_final, S_content\n",
    "\n",
    "def local_allowed_ids(df: pd.DataFrame, signals: Dict[str,np.ndarray], query_idx: int, radius_km: float, require_shared_language: bool):\n",
    "    Drow = signals[\"D\"][query_idx]\n",
    "    allowed = [i for i in range(len(df)) if i!=query_idx and Drow[i] <= radius_km]\n",
    "    if require_shared_language:\n",
    "        Lq = set([x.strip().lower() for x in df.iloc[query_idx].languages.split(\",\") if x.strip()])\n",
    "        allowed = [i for i in allowed if len(Lq & set([x.strip().lower() for x in df.iloc[i].languages.split(\",\") if x.strip()]))>0]\n",
    "    return allowed\n",
    "\n",
    "def network_expansion(df: pd.DataFrame, signals: Dict[str,np.ndarray], query_idx: int,\n",
    "                      k:int=10, skills_mode=\"similar\", search_mode=\"local\", local_radius_km=50.0,\n",
    "                      require_shared_language=True, diversifier=\"mmr\"):\n",
    "    S_final, _ = fused_matrix(signals, skills_mode=skills_mode)\n",
    "    if search_mode == \"local\":\n",
    "        allowed = local_allowed_ids(df, signals, query_idx, local_radius_km, require_shared_language)\n",
    "    else:\n",
    "        allowed = [i for i in range(len(df)) if i != query_idx]\n",
    "    if not allowed:\n",
    "        return pd.DataFrame(columns=[\"name\",\"location_city\",\"languages\",\"role\",\"score\"])\n",
    "\n",
    "    if diversifier==\"mmr\":\n",
    "        picks = mmr_rank(query_idx, S_final, K=min(k,len(allowed)), lambda_rel=0.72, allowed=allowed)\n",
    "    elif diversifier==\"dpp\":\n",
    "        picks = dpp_greedy(query_idx, S_final, K=min(k,len(allowed)), allowed=allowed)\n",
    "    else:\n",
    "        picks = sorted(allowed, key=lambda j: -S_final[query_idx,j])[:k]\n",
    "\n",
    "    cols = [\"name\",\"location_city\",\"languages\",\"role\",\"seniority\",\"interests\",\"skills_have\",\"skills_want\",\"years_exp\",\n",
    "            \"age\",\"age_band\",\"energy_1to5\",\"collab_style\",\"availability_hours\",\"reason\"]\n",
    "    out = df.iloc[picks][cols].copy()\n",
    "    out[\"score\"] = [S_final[query_idx,j] for j in picks]\n",
    "    return out\n",
    "\n",
    "def team_build(df: pd.DataFrame, signals: Dict[str,np.ndarray], query_idx: int,\n",
    "               skills_need_text:str, K:int=4, search_mode=\"local\", local_radius_km=50.0,\n",
    "               require_shared_language=True):\n",
    "    S_final, _ = fused_matrix(signals, skills_mode=\"complementary\")\n",
    "    need = [s.strip() for s in skills_need_text.split(\",\") if s.strip()]\n",
    "    if search_mode==\"local\":\n",
    "        allowed = set(local_allowed_ids(df, signals, query_idx, local_radius_km, require_shared_language))\n",
    "    else:\n",
    "        allowed = set([i for i in range(len(df)) if i != query_idx])\n",
    "    # Greedy selection over allowed only\n",
    "    selected = []\n",
    "    def team_score(set_ids: List[int]):\n",
    "        if not set_ids: return 0.0\n",
    "        rel = np.mean([S_final[query_idx, j] for j in set_ids])\n",
    "        need_set = set([s.strip().lower() for s in need if s.strip()])\n",
    "        have = set()\n",
    "        for j in set_ids:\n",
    "            have |= set([s.strip().lower() for s in df.iloc[j].skills_have.split(\",\") if s.strip()])\n",
    "        coverage = len(need_set & have) / (len(need_set) + 1e-9)\n",
    "        if len(set_ids) > 1:\n",
    "            pair_sims = []\n",
    "            for a in range(len(set_ids)):\n",
    "                for b in range(a+1, len(set_ids)):\n",
    "                    pair_sims.append(S_final[set_ids[a], set_ids[b]])\n",
    "            div = 1.0 - float(np.mean(pair_sims))\n",
    "        else:\n",
    "            div = 1.0\n",
    "        return 0.55*rel + 0.30*coverage + 0.15*div\n",
    "    candidates = list(allowed)\n",
    "    while candidates and len(selected) < K:\n",
    "        base = team_score(selected)\n",
    "        best_j, best_gain = None, -1\n",
    "        for j in candidates:\n",
    "            gain = team_score(selected+[j]) - base\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_j = gain, j\n",
    "        if best_j is None: break\n",
    "        selected.append(best_j); candidates.remove(best_j)\n",
    "\n",
    "    cols = ['name','location_city','languages','role','seniority','skills_have','years_exp','tz_offset','availability_hours','collab_style']\n",
    "    team_df = df.iloc[selected][cols].copy()\n",
    "    team_df['match_score'] = [S_final[query_idx,j] for j in selected]\n",
    "    return team_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1646ea",
   "metadata": {},
   "source": [
    "## Synthetic cohort & demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609070cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Network expansion (LOCAL Toronto, strict) ===\n",
      "   name location_city languages       role    score\n",
      "User144       Toronto    en, fr Researcher 0.781384\n",
      "User069       Toronto    fr, en  Scientist 0.695438\n",
      "User099       Toronto    fr, en    Creator 0.703937\n",
      "User092       Toronto    fr, en  Scientist 0.708148\n",
      "User061       Toronto        fr     Writer 0.705436\n",
      "User081       Toronto        fr   Engineer 0.665076\n",
      "User130       Toronto    fr, en    Creator 0.639348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roles = [\"Founder\",\"Engineer\",\"Designer\",\"Researcher\",\"Writer\",\"Scientist\",\"Creator\"]\n",
    "seniorities = [\"Junior\",\"Mid\",\"Senior\",\"Lead/Principal\",\"Executive/Founder\"]\n",
    "cities = [\n",
    "    (\"Toronto\",43.6532,-79.3832,-5, [\"en\",\"fr\"]),\n",
    "    (\"New York\",40.7128,-74.0060,-5, [\"en\",\"es\"]),\n",
    "    (\"San Francisco\",37.7749,-122.4194,-8, [\"en\",\"zh\"]),\n",
    "    (\"London\",51.5072,-0.1276,0, [\"en\",\"fr\",\"de\"]),\n",
    "    (\"Berlin\",52.52,13.405,1, [\"de\",\"en\"]),\n",
    "    (\"Nairobi\",-1.286389,36.817223,3, [\"en\",\"sw\"]),\n",
    "    (\"Sydney\",-33.8688,151.2093,10, [\"en\"]),\n",
    "    (\"Bangalore\",12.9716,77.5946,5, [\"en\",\"hi\"]),\n",
    "    (\"Paris\",48.8566,2.3522,1, [\"fr\",\"en\"]),\n",
    "    (\"Mexico City\",19.4326,-99.1332,-6, [\"es\",\"en\"])\n",
    "]\n",
    "\n",
    "def rand_words(pool, kmin, kmax):\n",
    "    k = random.randint(kmin, kmax)\n",
    "    return \", \".join(random.sample(list(pool), k))\n",
    "\n",
    "SKILL_LEXICON = set([\n",
    "    \"python\",\"pytorch\",\"tensorflow\",\"django\",\"react\",\"nextjs\",\"go\",\"kubernetes\",\"aws\",\"gcp\",\n",
    "    \"video editing\",\"storyboarding\",\"scriptwriting\",\"podcasting\",\"seo\",\"branding\",\"figma\",\"design systems\",\n",
    "    \"statistics\",\"causal inference\",\"nlp\",\"cv\",\"prompt engineering\",\"sql\",\"dbt\",\"airflow\",\n",
    "    \"grant writing\",\"field research\",\"lab techniques\",\"oceanography\",\"genomics\",\"biostatistics\",\n",
    "    \"supply chain\",\"marketing\",\"growth\",\"product\",\"fundraising\",\"strategy\"\n",
    "])\n",
    "interest_bank = [\n",
    "    \"ocean conservation\",\"coral reef restoration\",\"climate tech\",\"educational apps\",\"healthcare AI\",\n",
    "    \"creator economy\",\"open source tools\",\"social impact\",\"rural connectivity\",\"financial inclusion\",\n",
    "    \"short-form video\",\"long-form YouTube\",\"beauty brand\",\"lipstick R&D\",\"fashion sustainability\",\n",
    "    \"music production\",\"publishing\",\"newsletter growth\",\"sports analytics\",\"mental health\",\n",
    "    \"language learning\",\"VR social spaces\",\"next social network\",\"privacy-first messaging\"\n",
    "]\n",
    "\n",
    "def random_dob():\n",
    "    y = random.randint(1961, 2004)\n",
    "    m = random.randint(1,12); d = random.randint(1,28)\n",
    "    return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "\n",
    "def mk_user(i):\n",
    "    name = f\"User{i:03d}\"\n",
    "    (city, lat, lon, tz, langs_base) = random.choice(cities)\n",
    "    role = random.choice(roles)\n",
    "    seniority = random.choice(seniorities)\n",
    "    skills_have = rand_words(SKILL_LEXICON, 3, 7)\n",
    "    skills_want = rand_words(SKILL_LEXICON, 2, 5)\n",
    "    interests = rand_words(interest_bank, 3, 7)\n",
    "    years = random.randint(1, 18)\n",
    "    human = f\"I live in {city}. I like calm schedules and meetups; I enjoy running and cooking.\"\n",
    "    professional = f\"As a {role.lower()} with {years} years, I worked across startups and labs.\"\n",
    "    contributor = \"I prefer weekly demos and short design docs. I bring reliability and momentum.\"\n",
    "    interests_long = f\"Goals: {random.choice(['launch a YouTube channel on ML','build ocean microplastics sensors','start a cruelty-free lipstick brand','prototype a privacy-first social app'])}.\"\n",
    "    reason = random.choice([\"Find projects\",\"Expand network\",\"Find collaborators\",\"Build a dream\"])\n",
    "    langs = random.sample(langs_base, min(len(langs_base), random.choice([1,1,2])))\n",
    "    row = dict(\n",
    "        name=name, dob=random_dob(), location_city=city, location_country=\"\",\n",
    "        lat=lat, lon=lon, tz_offset=tz, languages=\", \".join(langs),\n",
    "        availability_hours=random.choice([\"2-5\",\"5-10\",\"10-20\",\"20+\"]),\n",
    "        energy_1to5=random.randint(1,5), collab_style=random.choice([\"async\",\"hybrid\",\"sync\"]),\n",
    "        role=role, seniority=seniority, years_exp=years,\n",
    "        skills_have=skills_have, skills_want=skills_want, interests=interests,\n",
    "        human=human, professional=professional, contributor=contributor, interests_long=interests_long, reason=reason\n",
    "    )\n",
    "    return normalize_intake(row)\n",
    "\n",
    "records = [mk_user(i) for i in range(1,151)]\n",
    "tipi_all = [[random.randint(2,6) for _ in range(10)] for __ in range(150)]\n",
    "bfs = [score_tipi(t) for t in tipi_all]\n",
    "users = pd.DataFrame(records)\n",
    "users['bf'] = bfs\n",
    "\n",
    "embedder = Embedder()\n",
    "signals = build_signals(users, embedder)\n",
    "\n",
    "# Toronto demo (STRICT): 50km radius + shared language\n",
    "tor_idx = users.index[users.location_city==\"Toronto\"][0]\n",
    "local_net = network_expansion(users, signals, tor_idx, k=8, skills_mode=\"similar\",\n",
    "                              search_mode=\"local\", local_radius_km=50.0, require_shared_language=True, diversifier=\"mmr\")\n",
    "\n",
    "print(\"=== Network expansion (LOCAL Toronto, strict) ===\")\n",
    "print(local_net[['name','location_city','languages','role','score']].to_string(index=False))\n",
    "\n",
    "# If you want to see fallback behavior (e.g., fewer than K available), uncomment:\n",
    "# print(f\"Returned {len(local_net)} rows (strict mode does not pad with remote users).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9b6e66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_band</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_country</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>years_exp</th>\n",
       "      <th>skills_have</th>\n",
       "      <th>skills_want</th>\n",
       "      <th>interests</th>\n",
       "      <th>human</th>\n",
       "      <th>professional</th>\n",
       "      <th>contributor</th>\n",
       "      <th>interests_long</th>\n",
       "      <th>reason</th>\n",
       "      <th>bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>User061</td>\n",
       "      <td>1963-11-26</td>\n",
       "      <td>61</td>\n",
       "      <td>55+</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>growth, podcasting, field research, design sys...</td>\n",
       "      <td>growth, strategy, storyboarding, fundraising, ...</td>\n",
       "      <td>privacy-first messaging, next social network, ...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a writer with 7 years, I worked across star...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.5, C=0.6666666666666667, E=0.66666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>User069</td>\n",
       "      <td>1964-09-02</td>\n",
       "      <td>60</td>\n",
       "      <td>55+</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr, en</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>sql, biostatistics, react, podcasting, pytorch...</td>\n",
       "      <td>branding, figma, storyboarding</td>\n",
       "      <td>educational apps, sports analytics, open sourc...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a scientist with 6 years, I worked across s...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Expand network</td>\n",
       "      <td>BigFive(O=0.16666666666666663, C=0.5, E=0.5, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>User081</td>\n",
       "      <td>1961-07-20</td>\n",
       "      <td>64</td>\n",
       "      <td>55+</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>nextjs, statistics, growth</td>\n",
       "      <td>tensorflow, storyboarding, product, grant writing</td>\n",
       "      <td>creator economy, music production, language le...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a engineer with 12 years, I worked across s...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Expand network</td>\n",
       "      <td>BigFive(O=0.4166666666666667, C=0.249999999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>User092</td>\n",
       "      <td>2003-07-27</td>\n",
       "      <td>22</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr, en</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>oceanography, video editing, figma</td>\n",
       "      <td>causal inference, product, python, seo, pytorch</td>\n",
       "      <td>educational apps, ocean conservation, fashion ...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a scientist with 9 years, I worked across s...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: start a cruelty-free lipstick brand.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.5833333333333334, C=0.583333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>User099</td>\n",
       "      <td>1991-08-23</td>\n",
       "      <td>33</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr, en</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>statistics, tensorflow, marketing, design systems</td>\n",
       "      <td>go, react, grant writing</td>\n",
       "      <td>language learning, newsletter growth, coral re...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a creator with 1 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Expand network</td>\n",
       "      <td>BigFive(O=0.6666666666666667, C=0.5, E=0.5, A=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>User130</td>\n",
       "      <td>1969-07-12</td>\n",
       "      <td>56</td>\n",
       "      <td>55+</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>fr, en</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>design systems, storyboarding, python, oceanog...</td>\n",
       "      <td>airflow, go, django, sql</td>\n",
       "      <td>healthcare AI, ocean conservation, publishing,...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a creator with 3 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: launch a YouTube channel on ML.</td>\n",
       "      <td>Build a dream</td>\n",
       "      <td>BigFive(O=0.4166666666666667, C=0.333333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>User144</td>\n",
       "      <td>1961-01-08</td>\n",
       "      <td>64</td>\n",
       "      <td>55+</td>\n",
       "      <td>Toronto</td>\n",
       "      <td></td>\n",
       "      <td>43.6532</td>\n",
       "      <td>-79.3832</td>\n",
       "      <td>-5</td>\n",
       "      <td>en, fr</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>dbt, gcp, branding, seo</td>\n",
       "      <td>biostatistics, product, strategy, grant writing</td>\n",
       "      <td>privacy-first messaging, social impact, ocean ...</td>\n",
       "      <td>I live in Toronto. I like calm schedules and m...</td>\n",
       "      <td>As a researcher with 16 years, I worked across...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: start a cruelty-free lipstick brand.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.6666666666666667, C=0.333333333333...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        name         dob  age age_band location_city location_country  \\\n",
       "60   User061  1963-11-26   61      55+       Toronto                    \n",
       "68   User069  1964-09-02   60      55+       Toronto                    \n",
       "80   User081  1961-07-20   64      55+       Toronto                    \n",
       "91   User092  2003-07-27   22    18-24       Toronto                    \n",
       "98   User099  1991-08-23   33    25-34       Toronto                    \n",
       "129  User130  1969-07-12   56      55+       Toronto                    \n",
       "143  User144  1961-01-08   64      55+       Toronto                    \n",
       "\n",
       "         lat      lon  tz_offset languages  ... years_exp  \\\n",
       "60   43.6532 -79.3832         -5        fr  ...         7   \n",
       "68   43.6532 -79.3832         -5    fr, en  ...         6   \n",
       "80   43.6532 -79.3832         -5        fr  ...        12   \n",
       "91   43.6532 -79.3832         -5    fr, en  ...         9   \n",
       "98   43.6532 -79.3832         -5    fr, en  ...         1   \n",
       "129  43.6532 -79.3832         -5    fr, en  ...         3   \n",
       "143  43.6532 -79.3832         -5    en, fr  ...        16   \n",
       "\n",
       "                                           skills_have  \\\n",
       "60   growth, podcasting, field research, design sys...   \n",
       "68   sql, biostatistics, react, podcasting, pytorch...   \n",
       "80                          nextjs, statistics, growth   \n",
       "91                  oceanography, video editing, figma   \n",
       "98   statistics, tensorflow, marketing, design systems   \n",
       "129  design systems, storyboarding, python, oceanog...   \n",
       "143                            dbt, gcp, branding, seo   \n",
       "\n",
       "                                           skills_want  \\\n",
       "60   growth, strategy, storyboarding, fundraising, ...   \n",
       "68                      branding, figma, storyboarding   \n",
       "80   tensorflow, storyboarding, product, grant writing   \n",
       "91     causal inference, product, python, seo, pytorch   \n",
       "98                            go, react, grant writing   \n",
       "129                           airflow, go, django, sql   \n",
       "143    biostatistics, product, strategy, grant writing   \n",
       "\n",
       "                                             interests  \\\n",
       "60   privacy-first messaging, next social network, ...   \n",
       "68   educational apps, sports analytics, open sourc...   \n",
       "80   creator economy, music production, language le...   \n",
       "91   educational apps, ocean conservation, fashion ...   \n",
       "98   language learning, newsletter growth, coral re...   \n",
       "129  healthcare AI, ocean conservation, publishing,...   \n",
       "143  privacy-first messaging, social impact, ocean ...   \n",
       "\n",
       "                                                 human  \\\n",
       "60   I live in Toronto. I like calm schedules and m...   \n",
       "68   I live in Toronto. I like calm schedules and m...   \n",
       "80   I live in Toronto. I like calm schedules and m...   \n",
       "91   I live in Toronto. I like calm schedules and m...   \n",
       "98   I live in Toronto. I like calm schedules and m...   \n",
       "129  I live in Toronto. I like calm schedules and m...   \n",
       "143  I live in Toronto. I like calm schedules and m...   \n",
       "\n",
       "                                          professional  \\\n",
       "60   As a writer with 7 years, I worked across star...   \n",
       "68   As a scientist with 6 years, I worked across s...   \n",
       "80   As a engineer with 12 years, I worked across s...   \n",
       "91   As a scientist with 9 years, I worked across s...   \n",
       "98   As a creator with 1 years, I worked across sta...   \n",
       "129  As a creator with 3 years, I worked across sta...   \n",
       "143  As a researcher with 16 years, I worked across...   \n",
       "\n",
       "                                           contributor  \\\n",
       "60   I prefer weekly demos and short design docs. I...   \n",
       "68   I prefer weekly demos and short design docs. I...   \n",
       "80   I prefer weekly demos and short design docs. I...   \n",
       "91   I prefer weekly demos and short design docs. I...   \n",
       "98   I prefer weekly demos and short design docs. I...   \n",
       "129  I prefer weekly demos and short design docs. I...   \n",
       "143  I prefer weekly demos and short design docs. I...   \n",
       "\n",
       "                                   interests_long          reason  \\\n",
       "60   Goals: prototype a privacy-first social app.   Find projects   \n",
       "68   Goals: prototype a privacy-first social app.  Expand network   \n",
       "80   Goals: prototype a privacy-first social app.  Expand network   \n",
       "91    Goals: start a cruelty-free lipstick brand.   Find projects   \n",
       "98   Goals: prototype a privacy-first social app.  Expand network   \n",
       "129        Goals: launch a YouTube channel on ML.   Build a dream   \n",
       "143   Goals: start a cruelty-free lipstick brand.   Find projects   \n",
       "\n",
       "                                                    bf  \n",
       "60   BigFive(O=0.5, C=0.6666666666666667, E=0.66666...  \n",
       "68   BigFive(O=0.16666666666666663, C=0.5, E=0.5, A...  \n",
       "80   BigFive(O=0.4166666666666667, C=0.249999999999...  \n",
       "91   BigFive(O=0.5833333333333334, C=0.583333333333...  \n",
       "98   BigFive(O=0.6666666666666667, C=0.5, E=0.5, A=...  \n",
       "129  BigFive(O=0.4166666666666667, C=0.333333333333...  \n",
       "143  BigFive(O=0.6666666666666667, C=0.333333333333...  \n",
       "\n",
       "[7 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.loc[users['name'].isin(local_net['name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b625d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
