{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee747b4",
   "metadata": {},
   "source": [
    "\n",
    "# Rocket — **City‑Aware, Language‑Aware** Matching & Team Formation (with CF + Bandits)\n",
    "\n",
    "This notebook is a production‑minded prototype for **networked team building**, optimized for **local teams** with a **global search** fallback.\n",
    "\n",
    "**What it does**\n",
    "- **Rich intake** with DOB→age, city/lat/lon & time zone, **languages (ISO codes)**, availability, energy, collab style, role/seniority/years, skills **have/want**, interests, 5 interview sections.\n",
    "- **Multilingual embeddings** for content (Sentence‑Transformers if available; TF‑IDF fallback).\n",
    "- **Geo + language constraints**: \n",
    "  - **Local mode**: restrict or sharply weight to same‑city / near‑city within a radius (km), and **require shared language**.\n",
    "  - **Global mode**: ignore geo or down‑weight it; language is a soft constraint.\n",
    "- **Skills**: similar vs **complementary** (Hungarian/coverage with fallback).\n",
    "- **Collaborative filtering**: lightweight MF (ALS‑ish) over implicit likes.\n",
    "- **Graph**: **Personalized PageRank** over the directed “likes” graph.\n",
    "- **Bandits**: **LinUCB** (contextual bandit) learns which blend weights to use per user based on context.\n",
    "- **Diversification**: MMR and DPP‑style greedy.\n",
    "- **Team formation**: greedy submodular objective balances match, **skill coverage**, and **diversity**, with hard constraints (distance, language, time zone, availability).\n",
    "\n",
    "> Final cell prints **ALL names** so you can verify cohort creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7d7a1",
   "metadata": {},
   "source": [
    "## Optional installs (run locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install numpy pandas scikit-learn networkx geopy scipy\n",
    "# !pip install sentence-transformers fasttext-langdetect langdetect\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, random, math, importlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import date, datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "import networkx as nx\n",
    "\n",
    "np.random.seed(2025); random.seed(2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e5230",
   "metadata": {},
   "source": [
    "## Embeddings — multilingual SBERT if available; TF‑IDF fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_names: List[str] = None):\n",
    "        self.model = None\n",
    "        self.sbert_ok = False\n",
    "        self.tfidf = None\n",
    "        self.model_names = model_names or [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        ]\n",
    "        if importlib.util.find_spec(\"sentence_transformers\") is not None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            for name in self.model_names:\n",
    "                try:\n",
    "                    self.model = SentenceTransformer(name)\n",
    "                    self.sbert_ok = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        if self.sbert_ok:\n",
    "            return self\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "        self.tfidf.fit(corpus)\n",
    "        return self\n",
    "\n",
    "    def encode(self, items: List[str]) -> np.ndarray:\n",
    "        if self.sbert_ok:\n",
    "            return np.array(self.model.encode(items, show_progress_bar=False, normalize_embeddings=True))\n",
    "        X = self.tfidf.transform(items)\n",
    "        X = X.astype(np.float64)\n",
    "        norms = np.sqrt((X.power(2)).sum(axis=1))\n",
    "        norms[norms==0] = 1.0\n",
    "        return (X / norms).toarray()\n",
    "\n",
    "def cos_sim_mat(A: np.ndarray) -> np.ndarray:\n",
    "    return (A @ A.T).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a12bc4",
   "metadata": {},
   "source": [
    "## Utilities — DOB → Age / Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_dob(dob_str: str) -> date:\n",
    "    return datetime.strptime(dob_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "def compute_age(dob: date, today: Optional[date] = None) -> int:\n",
    "    today = today or date.today()\n",
    "    years = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "    return max(0, years)\n",
    "\n",
    "def age_band(age: int) -> str:\n",
    "    for lo, hi in [(18,24),(25,34),(35,44),(45,54)]:\n",
    "        if lo <= age <= hi: return f\"{lo}-{hi}\"\n",
    "    return \"55+\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c3bbd",
   "metadata": {},
   "source": [
    "## Personality — TIPI (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class BigFive:\n",
    "    O: float; C: float; E: float; A: float; N: float\n",
    "\n",
    "def clip01(x): \n",
    "    import numpy as np\n",
    "    return float(np.clip(x, 0.0, 1.0))\n",
    "\n",
    "TIPI_KEY = {\n",
    "    1: (\"E\", False), 2: (\"A\", True), 3: (\"C\", False), 4: (\"N\", False), 5: (\"O\", False),\n",
    "    6: (\"E\", True),  7: (\"A\", False),8: (\"C\", True),  9: (\"N\", True),  10:(\"O\", True)\n",
    "}\n",
    "\n",
    "def score_tipi(responses_1to7):\n",
    "    import numpy as np\n",
    "    assert len(responses_1to7)==10\n",
    "    r = np.array(responses_1to7, dtype=float)\n",
    "    r01 = (r-1)/6.0\n",
    "    traits = {\"O\":[], \"C\":[], \"E\":[], \"A\":[], \"N\":[]}\n",
    "    for i,val in enumerate(r01, start=1):\n",
    "        trait, rev = TIPI_KEY[i]\n",
    "        traits[trait].append(1.0-val if rev else val)\n",
    "    return BigFive(*(clip01(np.mean(traits[t])) for t in [\"O\",\"C\",\"E\",\"A\",\"N\"]))\n",
    "\n",
    "def bigfive_cosine(u: BigFive, v: BigFive) -> float:\n",
    "    import numpy as np\n",
    "    a = np.array([u.O,u.C,u.E,u.A,u.N])\n",
    "    b = np.array([v.O,v.C,v.E,v.A,v.N])\n",
    "    return float(a @ b / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6216c",
   "metadata": {},
   "source": [
    "## Intake schema + normalizer (city & languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INTAKE_FIELDS = [\n",
    "    \"name\",\"dob\",\"location_city\",\"location_country\",\"lat\",\"lon\",\"tz_offset\",\n",
    "    \"languages\",\n",
    "    \"availability_hours\",\"energy_1to5\",\"collab_style\",\n",
    "    \"role\",\"seniority\",\"years_exp\",\n",
    "    \"skills_have\",\"skills_want\",\"interests\",\n",
    "    \"human\",\"professional\",\"contributor\",\"interests_long\",\"reason\"\n",
    "]\n",
    "\n",
    "def parse_comma_list(s: str) -> List[str]:\n",
    "    return [x.strip() for x in (s or \"\").split(\",\") if x.strip()]\n",
    "\n",
    "def normalize_intake(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    def wclip(t): \n",
    "        ws = (t or \"\").split()\n",
    "        return \" \".join(ws[:250])\n",
    "    dob_str = row.get(\"dob\",\"1989-01-01\")\n",
    "    try:\n",
    "        dob = parse_dob(dob_str)\n",
    "    except Exception:\n",
    "        dob = date(1989,1,1); dob_str=\"1989-01-01\"\n",
    "    age_val = compute_age(dob)\n",
    "    return {\n",
    "        \"name\": row.get(\"name\",\"Unnamed\"),\n",
    "        \"dob\": dob_str, \"age\": age_val, \"age_band\": age_band(age_val),\n",
    "        \"location_city\": row.get(\"location_city\",\"\"), \"location_country\": row.get(\"location_country\",\"\"),\n",
    "        \"lat\": float(row.get(\"lat\", 43.6532)), \"lon\": float(row.get(\"lon\", -79.3832)),\n",
    "        \"tz_offset\": int(row.get(\"tz_offset\", -5)),\n",
    "        \"languages\": \", \".join(parse_comma_list(row.get(\"languages\",\"en\")))[:64],\n",
    "        \"availability_hours\": row.get(\"availability_hours\",\"5-10\"),\n",
    "        \"energy_1to5\": int(row.get(\"energy_1to5\",3)),\n",
    "        \"collab_style\": row.get(\"collab_style\",\"hybrid\"),\n",
    "        \"role\": row.get(\"role\",\"Undecided\"),\n",
    "        \"seniority\": row.get(\"seniority\",\"Mid\"),\n",
    "        \"years_exp\": int(row.get(\"years_exp\",3)),\n",
    "        \"skills_have\": \", \".join(parse_comma_list(row.get(\"skills_have\",\"\"))[:24]),\n",
    "        \"skills_want\": \", \".join(parse_comma_list(row.get(\"skills_want\",row.get(\"interests\",\"\")))[:24]),\n",
    "        \"interests\": \", \".join(parse_comma_list(row.get(\"interests\",\"\"))[:24]),\n",
    "        \"human\": wclip(row.get(\"human\",\"\")),\n",
    "        \"professional\": wclip(row.get(\"professional\",\"\")),\n",
    "        \"contributor\": wclip(row.get(\"contributor\",\"\")),\n",
    "        \"interests_long\": wclip(row.get(\"interests_long\",\"\")),\n",
    "        \"reason\": wclip(row.get(\"reason\",\"\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43ebd6",
   "metadata": {},
   "source": [
    "## Feature builders — content, geo, experience, role, social‑fit, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_text_similarity(df: pd.DataFrame, embedder):\n",
    "    corpus = (df['interests'].fillna('') + \" ; \" + df['skills_have'].fillna('') + \" ; \" + df['professional'].fillna('')).tolist()\n",
    "    embedder.fit(corpus)\n",
    "    X = embedder.encode(corpus)\n",
    "    S = (X @ X.T)\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    return S\n",
    "\n",
    "def language_overlap(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    langs = [set([x.strip().lower() for x in (l or \"\").split(\",\") if x.strip()]) for l in df['languages'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            inter = langs[i] & langs[j]\n",
    "            uni = langs[i] | langs[j]\n",
    "            S[i,j] = len(inter)/float(len(uni) + 1e-9)\n",
    "    return S\n",
    "\n",
    "def geo_similarity(df: pd.DataFrame, decay_km: float = 1200.0) -> np.ndarray:\n",
    "    n = len(df); S = np.zeros((n,n), dtype=float)\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            d_km = geodesic(coords[i], coords[j]).km\n",
    "            S[i,j] = np.exp(-d_km/decay_km)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def distance_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); D=np.zeros((n,n))\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            D[i,j] = geodesic(coords[i], coords[j]).km\n",
    "    return D\n",
    "\n",
    "def experience_compatibility(years: List[int], sweet_spot: float = 3.0) -> np.ndarray:\n",
    "    years = np.array(years); n=len(years); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(years[i]-years[j])\n",
    "            S[i,j] = np.exp(-((gap-sweet_spot)**2)/(2*(sweet_spot**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "ROLE_COMP = {\n",
    "    \"Founder\": {\"Engineer\": 1.0, \"Designer\": 1.0, \"Researcher\": 0.8, \"Founder\": 0.2, \"Writer\":0.6, \"Scientist\":0.7, \"Creator\":0.8},\n",
    "    \"Engineer\": {\"Founder\": 1.0, \"Designer\": 0.7, \"Engineer\": 0.2, \"Researcher\": 0.6, \"Writer\":0.6, \"Scientist\":0.8, \"Creator\":0.7},\n",
    "    \"Designer\": {\"Founder\": 1.0, \"Engineer\": 0.7, \"Designer\": 0.2, \"Researcher\": 0.5, \"Writer\":0.6, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Researcher\": {\"Founder\": 0.8, \"Engineer\": 0.7, \"Designer\": 0.5, \"Researcher\": 0.3, \"Writer\":0.5, \"Scientist\":0.9, \"Creator\":0.6},\n",
    "    \"Writer\": {\"Founder\":0.8, \"Engineer\":0.6, \"Designer\":0.7, \"Researcher\":0.5, \"Writer\":0.2, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Scientist\":{\"Founder\":0.9, \"Engineer\":0.9, \"Designer\":0.5, \"Researcher\":0.8, \"Writer\":0.5, \"Scientist\":0.2, \"Creator\":0.6},\n",
    "    \"Creator\":{\"Founder\":0.9, \"Engineer\":0.7, \"Designer\":0.9, \"Researcher\":0.6, \"Writer\":0.9, \"Scientist\":0.6, \"Creator\":0.3},\n",
    "    \"Undecided\": {\"Founder\":0.6,\"Engineer\":0.6,\"Designer\":0.6,\"Researcher\":0.6,\"Writer\":0.6,\"Scientist\":0.6,\"Creator\":0.6,\"Undecided\":0.2}\n",
    "}\n",
    "\n",
    "def role_complementarity(df: pd.DataFrame) -> np.ndarray:\n",
    "    roles = df['role'].tolist(); n=len(roles); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = ROLE_COMP.get(roles[i], {}).get(roles[j], 0.2)\n",
    "    return S\n",
    "\n",
    "def energy_compatibility(energies: List[int], target_gap=0):\n",
    "    e = np.array(energies); n=len(e); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(e[i]-e[j])\n",
    "            S[i,j] = np.exp(-((gap-target_gap)**2)/(2*(1.25**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "COLLAB_COMP = {\n",
    "    \"async\": {\"async\":1.0, \"hybrid\":0.7, \"sync\":0.3},\n",
    "    \"hybrid\":{\"async\":0.7, \"hybrid\":1.0, \"sync\":0.7},\n",
    "    \"sync\":  {\"async\":0.3, \"hybrid\":0.7, \"sync\":1.0},\n",
    "}\n",
    "def collab_style_compatibility(styles: List[str]) -> np.ndarray:\n",
    "    n=len(styles); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = COLLAB_COMP.get(styles[i],{}).get(styles[j], 0.5)\n",
    "    return S\n",
    "\n",
    "def availability_overlap(avails: List[str]) -> np.ndarray:\n",
    "    map_mid = {\"2-5\":3.5,\"5-10\":7.5,\"10-20\":15.0,\"20+\":25.0}\n",
    "    v = np.array([map_mid.get(a,7.5) for a in avails])\n",
    "    n=len(v); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(v[i]-v[j])\n",
    "            S[i,j] = np.exp(-gap/15.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def time_zone_overlap(tz_list: List[int]) -> np.ndarray:\n",
    "    tz = np.array(tz_list); n=len(tz); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            diff = abs(tz[i]-tz[j])\n",
    "            S[i,j] = np.exp(-diff/6.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4510a8d",
   "metadata": {},
   "source": [
    "## Skills — similar vs complementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_skill_list(sk: str) -> List[str]:\n",
    "    return [s.strip().lower() for s in (sk or \"\").split(\",\") if s.strip()]\n",
    "\n",
    "def tfidf_cosine(a_list: List[str], b_list: List[str]) -> float:\n",
    "    docs = [\"; \".join(a_list), \"; \".join(b_list)]\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "    X = vec.fit_transform(docs)\n",
    "    return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "\n",
    "def similar_skills_matrix(df: pd.DataFrame, embedder=None) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    if embedder is not None and getattr(embedder, \"sbert_ok\", False):\n",
    "        corpus = df['skills_have'].fillna('').tolist()\n",
    "        X = embedder.encode(corpus)\n",
    "        S = (X @ X.T)\n",
    "        S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "        np.fill_diagonal(S, 0.0)\n",
    "        return S\n",
    "    parsed = [parse_skill_list(x) for x in df['skills_have'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = tfidf_cosine(parsed[i], parsed[j])\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def complementary_skills_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    wants = [parse_skill_list(row.get('skills_want', row.get('interests',''))) for _,row in df.iterrows()]\n",
    "    haves = [parse_skill_list(row.get('skills_have','')) for _,row in df.iterrows()]\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    try:\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                A = [\"; \".join([n1]) for n1 in need]\n",
    "                B = [\"; \".join([h1]) for h1 in have]\n",
    "                vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "                X = vec.fit_transform(A + B)\n",
    "                m, k = len(need), len(have)\n",
    "                Csim = np.zeros((m,k))\n",
    "                for p in range(m):\n",
    "                    for q in range(k):\n",
    "                        Csim[p,q] = cosine_similarity(X[p], X[m+q])[0,0]\n",
    "                size = max(m,k)\n",
    "                padded = np.ones((size,size))\n",
    "                padded[:m,:k] = 1.0 - Csim  # cost = 1 - sim\n",
    "                r_ind, c_ind = linear_sum_assignment(padded)\n",
    "                total_sim = 0.0; count = 0\n",
    "                for r,c in zip(r_ind, c_ind):\n",
    "                    if r < m and c < k:\n",
    "                        total_sim += 1.0 - padded[r,c]; count += 1\n",
    "                S[i,j] = total_sim / (count + 1e-9)\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    except Exception:\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                sims = []\n",
    "                for nterm in need:\n",
    "                    sims.append(max(tfidf_cosine([nterm], [h]) for h in have))\n",
    "                S[i,j] = float(np.mean(sims)) if sims else 0.0\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676577b5",
   "metadata": {},
   "source": [
    "## Collaborative filtering — lightweight MF (implicit feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mf_als(R: np.ndarray, k: int = 16, alpha: float = 40.0, reg: float = 0.1, iters: int = 6):\n",
    "    n_users, n_items = R.shape\n",
    "    C = 1 + alpha * R  # confidence\n",
    "    X = np.random.normal(scale=0.1, size=(n_users, k))\n",
    "    Y = np.random.normal(scale=0.1, size=(n_items, k))\n",
    "    eye = np.eye(k)\n",
    "    for _ in range(iters):\n",
    "        for u in range(n_users):\n",
    "            Cu = np.diag(C[u])\n",
    "            YTCuY = Y.T @ Cu @ Y\n",
    "            YTCuPu = Y.T @ (Cu @ R[u])\n",
    "            X[u] = np.linalg.solve(YTCuY + reg*eye, YTCuPu)\n",
    "        for i in range(n_items):\n",
    "            Ci = np.diag(C[:,i])\n",
    "            XTCiX = X.T @ Ci @ X\n",
    "            XTCiPi = X.T @ (Ci @ R[:,i])\n",
    "            Y[i] = np.linalg.solve(XTCiX + reg*eye, XTCiPi)\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    S = Xn @ Xn.T\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab8d93",
   "metadata": {},
   "source": [
    "## Graph (PPR), language/geo gating, and fusion (local vs global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reciprocalize(S: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(S * S.T + 1e-12)\n",
    "\n",
    "def build_graph_ppr(R: np.ndarray, alpha=0.85) -> np.ndarray:\n",
    "    n = R.shape[0]\n",
    "    G = nx.DiGraph(); G.add_nodes_from(range(n))\n",
    "    edges = [(u,v) for u in range(n) for v in range(n) if R[u,v]>0]\n",
    "    G.add_edges_from(edges)\n",
    "    S = np.zeros((n,n))\n",
    "    for u in range(n):\n",
    "        pr = nx.pagerank(G, alpha=alpha, personalization={k:(1.0 if k==u else 0.0) for k in range(n)})\n",
    "        for v,s in pr.items(): S[u,v] = s\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-12)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def combine_content(S_text, S_geo, S_exp, S_role, S_energy, S_collab, S_avail, S_tz, S_lang, w):\n",
    "    a,b,c,d,e,f,g,h,l = w\n",
    "    S = a*S_text + b*S_geo + c*S_exp + d*S_role + e*S_energy + f*S_collab + g*S_avail + h*S_tz + l*S_lang\n",
    "    return S / (S.max() + 1e-9)\n",
    "\n",
    "def fuse_scores(S_content, S_cf, S_graph, S_person, S_skills, weights=(0.30,0.18,0.16,0.10,0.26)):\n",
    "    Sc = reciprocalize(S_content)\n",
    "    Sf = reciprocalize(S_cf)\n",
    "    Sg = reciprocalize(S_graph)\n",
    "    Sp = reciprocalize(S_person)\n",
    "    Ss = reciprocalize(S_skills)\n",
    "    a,b,c,d,e = weights\n",
    "    S = a*Sc + b*Sf + c*Sg + d*Sp + e*Ss\n",
    "    return S / (S.max() + 1e-12)\n",
    "\n",
    "def distance_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); D=np.zeros((n,n))\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            D[i,j] = geodesic(coords[i], coords[j]).km\n",
    "    return D\n",
    "\n",
    "def apply_language_requirement(S: np.ndarray, S_lang: np.ndarray, require_shared: bool) -> np.ndarray:\n",
    "    if not require_shared: return S\n",
    "    mask = (S_lang > 0.0).astype(float)\n",
    "    return S * mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66abf30",
   "metadata": {},
   "source": [
    "## Diversification — MMR and DPP‑style greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mmr_rank(query_idx: int, S: np.ndarray, K: int = 5, lambda_rel: float = 0.7):\n",
    "    n = S.shape[0]\n",
    "    candidates = [i for i in range(n) if i != query_idx]\n",
    "    selected = []\n",
    "    while candidates and len(selected) < K:\n",
    "        if not selected:\n",
    "            i = max(candidates, key=lambda j: S[query_idx, j])\n",
    "            selected.append(i); candidates.remove(i)\n",
    "        else:\n",
    "            def score(j):\n",
    "                redundancy = max(S[j, s] for s in selected) if selected else 0.0\n",
    "                return lambda_rel * S[query_idx, j] - (1-lambda_rel) * redundancy\n",
    "            i = max(candidates, key=score)\n",
    "            selected.append(i); candidates.remove(i)\n",
    "    return selected\n",
    "\n",
    "def dpp_greedy(query_idx: int, S: np.ndarray, K: int = 5):\n",
    "    n = S.shape[0]\n",
    "    items = [i for i in range(n) if i != query_idx]\n",
    "    quality = S[query_idx].copy()\n",
    "    q = quality / (quality.max() + 1e-9)\n",
    "    selected = []\n",
    "    remaining = items.copy()\n",
    "    while remaining and len(selected) < K:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax([q[items.index(r)] for r in remaining]))\n",
    "            chosen = remaining[idx]\n",
    "        else:\n",
    "            scores = []\n",
    "            for r in remaining:\n",
    "                max_sim = max(S[r, s] for s in selected) if selected else 0.0\n",
    "                scores.append(q[items.index(r)] - max_sim)\n",
    "            idx = int(np.argmax(scores))\n",
    "            chosen = remaining[idx]\n",
    "        selected.append(chosen); remaining.pop(idx)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a88a0",
   "metadata": {},
   "source": [
    "## Contextual bandit — LinUCB (choose blend weights per‑user context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinUCB:\n",
    "    def __init__(self, arms: List[np.ndarray], alpha: float = 0.3, d: int = None):\n",
    "        self.arms = [np.array(a, dtype=float) for a in arms]\n",
    "        self.alpha = alpha\n",
    "        self.A = []\n",
    "        self.b = []\n",
    "        self.d = None if d is None else d\n",
    "        self.theta = []\n",
    "    def _init_if_needed(self, x: np.ndarray):\n",
    "        if self.d is None:\n",
    "            self.d = len(x)\n",
    "        if not self.A:\n",
    "            for _ in self.arms:\n",
    "                self.A.append(np.eye(self.d))\n",
    "                self.b.append(np.zeros(self.d))\n",
    "                self.theta.append(np.zeros(self.d))\n",
    "    def select(self, x: np.ndarray) -> int:\n",
    "        self._init_if_needed(x)\n",
    "        p = []\n",
    "        for i in range(len(self.arms)):\n",
    "            A_inv = np.linalg.inv(self.A[i])\n",
    "            theta = A_inv @ self.b[i]\n",
    "            self.theta[i] = theta\n",
    "            p_i = theta @ x + self.alpha * np.sqrt(x.T @ A_inv @ x)\n",
    "            p.append(p_i)\n",
    "        return int(np.argmax(p))\n",
    "    def update(self, arm_idx: int, x: np.ndarray, reward: float):\n",
    "        self._init_if_needed(x)\n",
    "        self.A[arm_idx] += np.outer(x, x)\n",
    "        self.b[arm_idx] += reward * x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454c3f9",
   "metadata": {},
   "source": [
    "## Generate 120 users across cities & languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roles = [\"Founder\",\"Engineer\",\"Designer\",\"Researcher\",\"Writer\",\"Scientist\",\"Creator\"]\n",
    "seniorities = [\"Junior\",\"Mid\",\"Senior\",\"Lead/Principal\",\"Executive/Founder\"]\n",
    "cities = [\n",
    "    (\"Toronto\",43.6532,-79.3832,-5, [\"en\",\"fr\"]),\n",
    "    (\"New York\",40.7128,-74.0060,-5, [\"en\",\"es\"]),\n",
    "    (\"San Francisco\",37.7749,-122.4194,-8, [\"en\",\"zh\"]),\n",
    "    (\"London\",51.5072,-0.1276,0, [\"en\",\"fr\",\"de\"]),\n",
    "    (\"Berlin\",52.52,13.405,1, [\"de\",\"en\"]),\n",
    "    (\"Nairobi\",-1.286389,36.817223,3, [\"en\",\"sw\"]),\n",
    "    (\"Sydney\",-33.8688,151.2093,10, [\"en\"]),\n",
    "    (\"Bangalore\",12.9716,77.5946,5, [\"en\",\"hi\"]),\n",
    "    (\"Paris\",48.8566,2.3522,1, [\"fr\",\"en\"]),\n",
    "    (\"Mexico City\",19.4326,-99.1332,-6, [\"es\",\"en\"])\n",
    "]\n",
    "\n",
    "skill_bank = [\n",
    "    \"python\",\"pytorch\",\"tensorflow\",\"django\",\"react\",\"nextjs\",\"go\",\"kubernetes\",\"aws\",\"gcp\",\n",
    "    \"video editing\",\"storyboarding\",\"scriptwriting\",\"podcasting\",\"seo\",\"branding\",\"figma\",\"design systems\",\n",
    "    \"statistics\",\"causal inference\",\"nlp\",\"cv\",\"prompt engineering\",\"sql\",\"dbt\",\"airflow\",\n",
    "    \"grant writing\",\"field research\",\"lab techniques\",\"oceanography\",\"genomics\",\"biostatistics\",\n",
    "    \"supply chain\",\"marketing\",\"growth\",\"product\",\"fundraising\",\"strategy\"\n",
    "]\n",
    "interest_bank = [\n",
    "    \"ocean conservation\",\"coral reef restoration\",\"climate tech\",\"educational apps\",\"healthcare AI\",\n",
    "    \"creator economy\",\"open source tools\",\"social impact\",\"rural connectivity\",\"financial inclusion\",\n",
    "    \"short-form video\",\"long-form YouTube\",\"beauty brand\",\"lipstick R&D\",\"fashion sustainability\",\n",
    "    \"music production\",\"publishing\",\"newsletter growth\",\"sports analytics\",\"mental health\",\n",
    "    \"language learning\",\"VR social spaces\",\"next social network\",\"privacy-first messaging\"\n",
    "]\n",
    "\n",
    "def rand_words(pool, kmin, kmax):\n",
    "    k = random.randint(kmin, kmax)\n",
    "    return \", \".join(random.sample(pool, k))\n",
    "\n",
    "def random_dob():\n",
    "    y = random.randint(1961, 2004)\n",
    "    m = random.randint(1,12); d = random.randint(1,28)\n",
    "    return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "\n",
    "def mk_user(i):\n",
    "    name = f\"User{i:03d}\"\n",
    "    (city, lat, lon, tz, langs_base) = random.choice(cities)\n",
    "    role = random.choice(roles)\n",
    "    seniority = random.choice(seniorities)\n",
    "    skills_have = rand_words(skill_bank, 3, 7)\n",
    "    skills_want = rand_words(skill_bank, 2, 5)\n",
    "    interests = rand_words(interest_bank, 3, 7)\n",
    "    years = random.randint(1, 18)\n",
    "    human = f\"I live in {city}. I like calm schedules and meetups; I enjoy running and cooking.\"\n",
    "    professional = f\"As a {role.lower()} with {years} years, I worked across startups and labs. I can produce prototypes, brand systems, docs, and production code.\"\n",
    "    contributor = \"I prefer weekly demos and short design docs. I bring reliability, curiosity, and momentum to small teams with clear ownership.\"\n",
    "    interests_long = f\"Goals: {random.choice(['launch a YouTube channel on ML','build ocean microplastics sensors','start a cruelty-free lipstick brand','prototype a privacy-first social app'])}.\"\n",
    "    reason = random.choice([\"Find projects\",\"Expand network\",\"Find collaborators\",\"Build a dream\"])\n",
    "\n",
    "    langs = random.sample(langs_base, min(len(langs_base), random.choice([1,1,2])))\n",
    "    row = dict(\n",
    "        name=name, dob=random_dob(), location_city=city, location_country=\"\",\n",
    "        lat=lat, lon=lon, tz_offset=tz, languages=\", \".join(langs),\n",
    "        availability_hours=random.choice([\"2-5\",\"5-10\",\"10-20\",\"20+\"]),\n",
    "        energy_1to5=random.randint(1,5), collab_style=random.choice([\"async\",\"hybrid\",\"sync\"]),\n",
    "        role=role, seniority=seniority, years_exp=years,\n",
    "        skills_have=skills_have, skills_want=skills_want, interests=interests,\n",
    "        human=human, professional=professional, contributor=contributor, interests_long=interests_long, reason=reason\n",
    "    )\n",
    "    return normalize_intake(row)\n",
    "\n",
    "records = [mk_user(i) for i in range(1,121)]\n",
    "tipi_all = [[random.randint(2,6) for _ in range(10)] for __ in range(120)]\n",
    "bfs = [score_tipi(t) for t in tipi_all]\n",
    "users = pd.DataFrame(records)\n",
    "users['bf'] = bfs\n",
    "users.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adc725",
   "metadata": {},
   "source": [
    "## Build similarity signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedder = Embedder()\n",
    "S_text = build_text_similarity(users, embedder)\n",
    "S_lang = language_overlap(users)\n",
    "S_geo  = geo_similarity(users, decay_km=1200.0)\n",
    "S_exp  = experience_compatibility(users['years_exp'].tolist())\n",
    "S_role = role_complementarity(users)\n",
    "S_energy = energy_compatibility(users['energy_1to5'].tolist())\n",
    "S_collab = collab_style_compatibility(users['collab_style'].tolist())\n",
    "S_avail  = availability_overlap(users['availability_hours'].tolist())\n",
    "S_tz     = time_zone_overlap(users['tz_offset'].tolist())\n",
    "D_km     = distance_matrix(users)\n",
    "\n",
    "S_skills_sim = similar_skills_matrix(users, embedder if getattr(embedder,\"sbert_ok\",False) else None)\n",
    "S_skills_comp = complementary_skills_matrix(users)\n",
    "\n",
    "n = len(users); R = np.zeros((n,n), dtype=float)\n",
    "for _ in range(900):\n",
    "    u = random.randrange(n); v = random.randrange(n)\n",
    "    if u==v: continue\n",
    "    if users.iloc[u].role==\"Founder\" and users.iloc[v].role in [\"Engineer\",\"Designer\"]: R[u,v]=1.0\n",
    "    elif users.iloc[u].role==\"Creator\" and users.iloc[v].role in [\"Writer\",\"Designer\",\"Engineer\"]: R[u,v]=1.0\n",
    "    elif users.iloc[u].location_city==users.iloc[v].location_city and random.random()<0.25: R[u,v]=1.0\n",
    "    elif random.random() < 0.04: R[u,v]=1.0\n",
    "\n",
    "S_cf = mf_als(R, k=16)\n",
    "S_graph = build_graph_ppr(R, alpha=0.82)\n",
    "\n",
    "S_person = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i==j: continue\n",
    "        S_person[i,j] = bigfive_cosine(users.iloc[i].bf, users.iloc[j].bf)\n",
    "S_person = (S_person - S_person.min())/(S_person.max()-S_person.min()+1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee848e",
   "metadata": {},
   "source": [
    "## Matching functions — local/global toggle + language requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_candidates(query_idx: int, skills_mode=\"similar\", k=5, search_mode=\"local\", \n",
    "                     local_radius_km: float = 50.0, require_shared_language: bool = True,\n",
    "                     diversifier=\"mmr\", content_weights=(0.24,0.16,0.10,0.10,0.10,0.10,0.10,0.05,0.05),\n",
    "                     blend_weights=(0.34,0.18,0.14,0.12,0.22)):\n",
    "    S_content = combine_content(S_text, S_geo, S_exp, S_role, S_energy, S_collab, S_avail, S_tz, S_lang, content_weights)\n",
    "    S_sk = S_skills_sim if skills_mode==\"similar\" else S_skills_comp\n",
    "    S_final = fuse_scores(S_content, S_cf, S_graph, S_person, S_skills=S_sk, weights=blend_weights)\n",
    "\n",
    "    if search_mode == \"local\":\n",
    "        # Mask out users beyond radius\n",
    "        mask_row = (D_km[query_idx] <= local_radius_km).astype(float)\n",
    "        S_final = S_final * mask_row.reshape(1,-1)\n",
    "        if require_shared_language:\n",
    "            S_final = apply_language_requirement(S_final, S_lang, True)\n",
    "\n",
    "    if diversifier == \"mmr\":\n",
    "        picks = mmr_rank(query_idx, S_final, K=k, lambda_rel=0.72)\n",
    "    elif diversifier == \"dpp\":\n",
    "        picks = dpp_greedy(query_idx, S_final, K=k)\n",
    "    else:\n",
    "        scores = list(enumerate(S_final[query_idx]))\n",
    "        scores = [(j,s) for j,s in scores if j!=query_idx]\n",
    "        picks = [j for j,_ in sorted(scores, key=lambda x: -x[1])[:k]]\n",
    "\n",
    "    cols = ['name','location_city','languages','role','seniority','interests','skills_have','skills_want','years_exp',\n",
    "            'age','age_band','energy_1to5','collab_style','availability_hours','reason']\n",
    "    out = users.iloc[picks][cols].copy()\n",
    "    out['score'] = [S_final[query_idx,j] for j in picks]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3ae45",
   "metadata": {},
   "source": [
    "## Team formation — local constraints (distance, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def team_score(set_ids: List[int], query_idx: int, S_final: np.ndarray, skills_need: List[str], users_df: pd.DataFrame):\n",
    "    if not set_ids: return 0.0\n",
    "    rel = np.mean([S_final[query_idx, j] for j in set_ids])\n",
    "    need = set([s.strip().lower() for s in skills_need if s.strip()])\n",
    "    have = set()\n",
    "    for j in set_ids:\n",
    "        have |= set([s.strip().lower() for s in users_df.iloc[j].skills_have.split(\",\") if s.strip()])\n",
    "    coverage = len(need & have) / (len(need) + 1e-9)\n",
    "    if len(set_ids) > 1:\n",
    "        pair_sims = []\n",
    "        for a in range(len(set_ids)):\n",
    "            for b in range(a+1, len(set_ids)):\n",
    "                pair_sims.append(S_final[set_ids[a], set_ids[b]])\n",
    "        div = 1.0 - float(np.mean(pair_sims))\n",
    "    else:\n",
    "        div = 1.0\n",
    "    return 0.55*rel + 0.30*coverage + 0.15*div\n",
    "\n",
    "def form_team(query_idx: int, skills_need_text: str, K: int = 4, search_mode=\"local\",\n",
    "              local_radius_km=50.0, require_shared_language=True, blend_weights=(0.34,0.18,0.14,0.12,0.22)):\n",
    "    need = [s.strip() for s in skills_need_text.split(\",\") if s.strip()]\n",
    "    S_content = combine_content(S_text, S_geo, S_exp, S_role, S_energy, S_collab, S_avail, S_tz, S_lang,\n",
    "                                (0.24,0.20,0.10,0.10,0.10,0.10,0.08,0.04,0.04))\n",
    "    S_sk = S_skills_comp\n",
    "    S_final = fuse_scores(S_content, S_cf, S_graph, S_person, S_skills=S_sk, weights=blend_weights)\n",
    "\n",
    "    candidates = [i for i in range(len(users)) if i != query_idx]\n",
    "    feasible = []\n",
    "    for j in candidates:\n",
    "        ok = True\n",
    "        if search_mode==\"local\" and D_km[query_idx,j] > local_radius_km: ok = False\n",
    "        if require_shared_language:\n",
    "            langs_i = set([x.strip().lower() for x in users.iloc[query_idx].languages.split(\",\") if x.strip()])\n",
    "            langs_j = set([x.strip().lower() for x in users.iloc[j].languages.split(\",\") if x.strip()])\n",
    "            if len(langs_i & langs_j)==0: ok=False\n",
    "        if ok: feasible.append(j)\n",
    "\n",
    "    selected = []\n",
    "    while feasible and len(selected) < K:\n",
    "        base = team_score(selected, query_idx, S_final, need, users)\n",
    "        best_gain, best_j = -1, None\n",
    "        for j in feasible:\n",
    "            gain = team_score(selected+[j], query_idx, S_final, need, users) - base\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_j = gain, j\n",
    "        if best_j is None: break\n",
    "        selected.append(best_j); feasible.remove(best_j)\n",
    "\n",
    "    cols = ['name','location_city','languages','role','seniority','skills_have','years_exp','tz_offset','availability_hours','collab_style']\n",
    "    df = users.iloc[selected][cols].copy()\n",
    "    df['match_score'] = [S_final[query_idx,j] for j in selected]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56755d",
   "metadata": {},
   "source": [
    "## LinUCB demo — learn which blend weights to use for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arms = [\n",
    "    np.array([0.40,0.18,0.10,0.10,0.22]),\n",
    "    np.array([0.28,0.26,0.16,0.10,0.20]),\n",
    "    np.array([0.32,0.18,0.14,0.16,0.20]),\n",
    "    np.array([0.30,0.18,0.12,0.10,0.30]),\n",
    "]\n",
    "linucb = LinUCB(arms, alpha=0.35)\n",
    "\n",
    "def user_context_vector(idx: int) -> np.ndarray:\n",
    "    e = users.iloc[idx].energy_1to5 / 5.0\n",
    "    tz = (users.iloc[idx].tz_offset + 12) / 24.0\n",
    "    exp = min(users.iloc[idx].years_exp, 20) / 20.0\n",
    "    role_onehot = np.zeros(7); role_map = {\"Founder\":0,\"Engineer\":1,\"Designer\":2,\"Researcher\":3,\"Writer\":4,\"Scientist\":5,\"Creator\":6}\n",
    "    role_onehot[role_map.get(users.iloc[idx].role,0)] = 1.0\n",
    "    return np.concatenate([[e,tz,exp], role_onehot])\n",
    "\n",
    "q = 0\n",
    "for t in range(10):\n",
    "    x = user_context_vector(q)\n",
    "    arm_idx = linucb.select(x)\n",
    "    res = match_candidates(q, skills_mode=\"similar\", k=5, search_mode=\"global\", blend_weights=arms[arm_idx])\n",
    "    top_idx = res.index[0]\n",
    "    city_ok = users.iloc[q].location_city == users.iloc[top_idx].location_city\n",
    "    lang_ok = len(set(users.iloc[q].languages.split(\", \")) & set(users.iloc[top_idx].languages.split(\", \")))>0\n",
    "    reward = 1.0 if (city_ok or lang_ok) else 0.0\n",
    "    linucb.update(arm_idx, x, reward)\n",
    "\n",
    "linucb.theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec46ca",
   "metadata": {},
   "source": [
    "## Demo — local vs global matches & local team build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mx_idx = users.index[users.location_city==\"Mexico City\"][0]\n",
    "ny_idx = users.index[users.location_city==\"New York\"][0]\n",
    "\n",
    "print(\"=== LOCAL (Mexico City) — matches (require shared language) ===\")\n",
    "mx_local = match_candidates(mx_idx, skills_mode=\"complementary\", k=5, search_mode=\"local\",\n",
    "                            local_radius_km=60.0, require_shared_language=True, diversifier=\"mmr\")\n",
    "print(mx_local[['name','location_city','languages','role','score']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== GLOBAL (Mexico City) — matches (no geo gating) ===\")\n",
    "mx_global = match_candidates(mx_idx, skills_mode=\"similar\", k=5, search_mode=\"global\",\n",
    "                             diversifier=\"dpp\")\n",
    "print(mx_global[['name','location_city','languages','role','score']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== LOCAL TEAM (New York) — need: react, product, branding, growth ===\")\n",
    "ny_team = form_team(ny_idx, \"react, product, branding, growth\", K=4, search_mode=\"local\",\n",
    "                    local_radius_km=50.0, require_shared_language=True)\n",
    "print(ny_team.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a95342",
   "metadata": {},
   "source": [
    "## Final — print ALL names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135767bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_names_df = users[['name']].copy()\n",
    "print(all_names_df.to_string(index=False))\n",
    "all_names_df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}