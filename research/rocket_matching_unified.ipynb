{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43df008c",
   "metadata": {},
   "source": [
    "\n",
    "# Rocket — **Unified Matching & Team Formation Notebook**\n",
    "\n",
    "**Goals covered**\n",
    "1. **Expand network:** Find people with **similar skills & interests** who also fit well **professionally and personally**.\n",
    "2. **Build teams (local or global):** Form skill‑complementary teams to solve a specific problem/brief, respecting **city radius**, **shared language**, **time zone**, **availability**, **energy** and **collab style**.\n",
    "3. **Field / passion discovery:** Find users **interested in a given field** and **demonstrate their skills** and relevant experience.\n",
    "\n",
    "**What’s inside (prod‑minded)**\n",
    "- Multilingual semantic embeddings (Sentence‑Transformers if available; TF‑IDF fallback).\n",
    "- Content similarity, skill similarity & complementarity (Hungarian coverage).\n",
    "- Geo & language awareness (local vs global mode; hard/soft gates).\n",
    "- Social fit (energy, collab style, time‑zone, availability), role complementarity, experience compatibility.\n",
    "- Collaborative filtering (implicit MF/ALS‑style), graph lift (Personalized PageRank), personality match (TIPI→Big Five).\n",
    "- Rank diversification (MMR / DPP‑style greedy).\n",
    "- Team formation via greedy **submodular** objective (relevance + skills coverage + diversity) with constraints.\n",
    "- **Contextual bandit (LinUCB)** to adapt blending weights per‑user context (optional demo).\n",
    "- **Resume/free‑text extraction** helpers (rule‑based; optional spaCy if installed).\n",
    "- Synthetic data generator across cities & languages for instant demos.\n",
    "- Final cell prints **ALL user names** for quick sanity check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58486e60",
   "metadata": {},
   "source": [
    "## Optional installs (run locally if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e795b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install numpy pandas scikit-learn networkx geopy scipy\n",
    "# !pip install sentence-transformers\n",
    "# !pip install spacy python-docx pdfplumber\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b851dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, random, math, importlib, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import date, datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "import networkx as nx\n",
    "\n",
    "np.random.seed(4242); random.seed(4242)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52daf264",
   "metadata": {},
   "source": [
    "## Embeddings — multilingual SBERT preferred; TF‑IDF fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eadc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_names: List[str] = None):\n",
    "        self.model = None\n",
    "        self.sbert_ok = False\n",
    "        self.tfidf = None\n",
    "        self.model_names = model_names or [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        ]\n",
    "        if importlib.util.find_spec(\"sentence_transformers\") is not None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            for name in self.model_names:\n",
    "                try:\n",
    "                    self.model = SentenceTransformer(name)\n",
    "                    self.sbert_ok = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        if self.sbert_ok:\n",
    "            return self\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "        self.tfidf.fit(corpus)\n",
    "        return self\n",
    "\n",
    "    def encode(self, items: List[str]) -> np.ndarray:\n",
    "        if self.sbert_ok:\n",
    "            return np.array(self.model.encode(items, show_progress_bar=False, normalize_embeddings=True))\n",
    "        X = self.tfidf.transform(items)\n",
    "        X = X.astype(np.float64)\n",
    "        norms = np.sqrt((X.power(2)).sum(axis=1))\n",
    "        norms[norms==0] = 1.0\n",
    "        return (X / norms).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ff2ef",
   "metadata": {},
   "source": [
    "## Utilities — DOB→Age + TIPI personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027e320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_dob(dob_str: str) -> date:\n",
    "    return datetime.strptime(dob_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "def compute_age(dob: date, today: Optional[date] = None) -> int:\n",
    "    today = today or date.today()\n",
    "    years = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "    return max(0, years)\n",
    "\n",
    "def age_band(age: int) -> str:\n",
    "    for lo, hi in [(18,24),(25,34),(35,44),(45,54)]:\n",
    "        if lo <= age <= hi: return f\"{lo}-{hi}\"\n",
    "    return \"55+\"\n",
    "\n",
    "@dataclass\n",
    "class BigFive:\n",
    "    O: float; C: float; E: float; A: float; N: float\n",
    "\n",
    "def clip01(x): \n",
    "    import numpy as np\n",
    "    return float(np.clip(x, 0.0, 1.0))\n",
    "\n",
    "TIPI_KEY = {\n",
    "    1: (\"E\", False), 2: (\"A\", True), 3: (\"C\", False), 4: (\"N\", False), 5: (\"O\", False),\n",
    "    6: (\"E\", True),  7: (\"A\", False),8: (\"C\", True),  9: (\"N\", True),  10:(\"O\", True)\n",
    "}\n",
    "\n",
    "def score_tipi(responses_1to7):\n",
    "    import numpy as np\n",
    "    assert len(responses_1to7)==10\n",
    "    r = np.array(responses_1to7, dtype=float)\n",
    "    r01 = (r-1)/6.0  # 1..7 -> 0..1\n",
    "    traits = {\"O\":[], \"C\":[], \"E\":[], \"A\":[], \"N\":[]}\n",
    "    for i,val in enumerate(r01, start=1):\n",
    "        trait, rev = TIPI_KEY[i]\n",
    "        traits[trait].append(1.0-val if rev else val)\n",
    "    return BigFive(*(clip01(np.mean(traits[t])) for t in [\"O\",\"C\",\"E\",\"A\",\"N\"]))\n",
    "\n",
    "def bigfive_cosine(u: BigFive, v: BigFive) -> float:\n",
    "    import numpy as np\n",
    "    a = np.array([u.O,u.C,u.E,u.A,u.N])\n",
    "    b = np.array([v.O,v.C,v.E,v.A,v.N])\n",
    "    return float(a @ b / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86e81f",
   "metadata": {},
   "source": [
    "## Intake schema + normalizer + resume/free‑text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bf9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INTAKE_FIELDS = [\n",
    "    \"name\",\"dob\",\"location_city\",\"location_country\",\"lat\",\"lon\",\"tz_offset\",\n",
    "    \"languages\",\"availability_hours\",\"energy_1to5\",\"collab_style\",\n",
    "    \"role\",\"seniority\",\"years_exp\",\n",
    "    \"skills_have\",\"skills_want\",\"interests\",\n",
    "    \"human\",\"professional\",\"contributor\",\"interests_long\",\"reason\"\n",
    "]\n",
    "\n",
    "def parse_comma_list(s: str) -> List[str]:\n",
    "    return [x.strip() for x in (s or \"\").split(\",\") if x.strip()]\n",
    "\n",
    "def normalize_intake(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    def wclip(t): \n",
    "        ws = (t or \"\").split()\n",
    "        return \" \".join(ws[:250])\n",
    "    dob_str = row.get(\"dob\",\"1989-01-01\")\n",
    "    try:\n",
    "        dob = parse_dob(dob_str)\n",
    "    except Exception:\n",
    "        dob = date(1989,1,1); dob_str=\"1989-01-01\"\n",
    "    age_val = compute_age(dob)\n",
    "    return {\n",
    "        \"name\": row.get(\"name\",\"Unnamed\"),\n",
    "        \"dob\": dob_str, \"age\": age_val, \"age_band\": age_band(age_val),\n",
    "        \"location_city\": row.get(\"location_city\",\"\"), \"location_country\": row.get(\"location_country\",\"\"),\n",
    "        \"lat\": float(row.get(\"lat\", 43.6532)), \"lon\": float(row.get(\"lon\", -79.3832)),\n",
    "        \"tz_offset\": int(row.get(\"tz_offset\", -5)),\n",
    "        \"languages\": \", \".join(parse_comma_list(row.get(\"languages\",\"en\")))[:64],\n",
    "        \"availability_hours\": row.get(\"availability_hours\",\"5-10\"),\n",
    "        \"energy_1to5\": int(row.get(\"energy_1to5\",3)),\n",
    "        \"collab_style\": row.get(\"collab_style\",\"hybrid\"),\n",
    "        \"role\": row.get(\"role\",\"Undecided\"),\n",
    "        \"seniority\": row.get(\"seniority\",\"Mid\"),\n",
    "        \"years_exp\": int(row.get(\"years_exp\",3)),\n",
    "        \"skills_have\": \", \".join(parse_comma_list(row.get(\"skills_have\",\"\"))[:24]),\n",
    "        \"skills_want\": \", \".join(parse_comma_list(row.get(\"skills_want\",row.get(\"interests\",\"\")))[:24]),\n",
    "        \"interests\": \", \".join(parse_comma_list(row.get(\"interests\",\"\"))[:24]),\n",
    "        \"human\": wclip(row.get(\"human\",\"\")),\n",
    "        \"professional\": wclip(row.get(\"professional\",\"\")),\n",
    "        \"contributor\": wclip(row.get(\"contributor\",\"\")),\n",
    "        \"interests_long\": wclip(row.get(\"interests_long\",\"\")),\n",
    "        \"reason\": wclip(row.get(\"reason\",\"\")),\n",
    "    }\n",
    "\n",
    "SKILL_LEXICON = set([\n",
    "    \"python\",\"pytorch\",\"tensorflow\",\"django\",\"react\",\"nextjs\",\"go\",\"kubernetes\",\"aws\",\"gcp\",\n",
    "    \"video editing\",\"storyboarding\",\"scriptwriting\",\"podcasting\",\"seo\",\"branding\",\"figma\",\"design systems\",\n",
    "    \"statistics\",\"causal inference\",\"nlp\",\"cv\",\"prompt engineering\",\"sql\",\"dbt\",\"airflow\",\n",
    "    \"grant writing\",\"field research\",\"lab techniques\",\"oceanography\",\"genomics\",\"biostatistics\",\n",
    "    \"supply chain\",\"marketing\",\"growth\",\"product\",\"fundraising\",\"strategy\"\n",
    "])\n",
    "\n",
    "def extract_from_text(text: str) -> Dict[str, Any]:\n",
    "    # very light rule-based extractor (plug in spaCy if available)\n",
    "    maybe_city = re.findall(r\"\\b(?:Toronto|New York|San Francisco|London|Berlin|Nairobi|Sydney|Bangalore|Paris|Mexico City)\\b\", text)\n",
    "    city = maybe_city[0] if maybe_city else \"\"\n",
    "    years = None\n",
    "    m = re.search(r\"(\\d{1,2})\\s*(?:\\+?\\s*)?(?:years|yrs)\", text, flags=re.I)\n",
    "    if m:\n",
    "        years = int(m.group(1))\n",
    "    skills = set()\n",
    "    for token in SKILL_LEXICON:\n",
    "        if re.search(rf\"\\b{re.escape(token)}\\b\", text, flags=re.I):\n",
    "            skills.add(token)\n",
    "    return {\n",
    "        \"location_city\": city,\n",
    "        \"years_exp\": years if years is not None else 3,\n",
    "        \"skills_have\": \", \".join(sorted(skills))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95a670",
   "metadata": {},
   "source": [
    "## Feature builders — content, geo, experience, role, social‑fit, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df452f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_text_similarity(df: pd.DataFrame, embedder):\n",
    "    corpus = (df['interests'].fillna('') + \" ; \" + df['skills_have'].fillna('') + \" ; \" + df['professional'].fillna('')).tolist()\n",
    "    embedder.fit(corpus)\n",
    "    X = embedder.encode(corpus)\n",
    "    S = (X @ X.T)\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    return S\n",
    "\n",
    "def language_overlap(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    langs = [set([x.strip().lower() for x in (l or \"\").split(\",\") if x.strip()]) for l in df['languages'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            inter = langs[i] & langs[j]\n",
    "            uni = langs[i] | langs[j]\n",
    "            S[i,j] = len(inter)/float(len(uni) + 1e-9)\n",
    "    return S\n",
    "\n",
    "def geo_similarity(df: pd.DataFrame, decay_km: float = 1200.0) -> np.ndarray:\n",
    "    n = len(df); S = np.zeros((n,n), dtype=float)\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            d_km = geodesic(coords[i], coords[j]).km\n",
    "            S[i,j] = np.exp(-d_km/decay_km)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def distance_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    n=len(df); D=np.zeros((n,n))\n",
    "    coords = list(zip(df['lat'], df['lon']))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            D[i,j] = geodesic(coords[i], coords[j]).km\n",
    "    return D\n",
    "\n",
    "def experience_compatibility(years: List[int], sweet_spot: float = 3.0) -> np.ndarray:\n",
    "    years = np.array(years); n=len(years); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(years[i]-years[j])\n",
    "            S[i,j] = np.exp(-((gap-sweet_spot)**2)/(2*(sweet_spot**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "ROLE_COMP = {\n",
    "    \"Founder\": {\"Engineer\": 1.0, \"Designer\": 1.0, \"Researcher\": 0.8, \"Founder\": 0.2, \"Writer\":0.6, \"Scientist\":0.7, \"Creator\":0.8},\n",
    "    \"Engineer\": {\"Founder\": 1.0, \"Designer\": 0.7, \"Engineer\": 0.2, \"Researcher\": 0.6, \"Writer\":0.6, \"Scientist\":0.8, \"Creator\":0.7},\n",
    "    \"Designer\": {\"Founder\": 1.0, \"Engineer\": 0.7, \"Designer\": 0.2, \"Researcher\": 0.5, \"Writer\":0.6, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Researcher\": {\"Founder\": 0.8, \"Engineer\": 0.7, \"Designer\": 0.5, \"Researcher\": 0.3, \"Writer\":0.5, \"Scientist\":0.9, \"Creator\":0.6},\n",
    "    \"Writer\": {\"Founder\":0.8, \"Engineer\":0.6, \"Designer\":0.7, \"Researcher\":0.5, \"Writer\":0.2, \"Scientist\":0.5, \"Creator\":0.9},\n",
    "    \"Scientist\":{\"Founder\":0.9, \"Engineer\":0.9, \"Designer\":0.5, \"Researcher\":0.8, \"Writer\":0.5, \"Scientist\":0.2, \"Creator\":0.6},\n",
    "    \"Creator\":{\"Founder\":0.9, \"Engineer\":0.7, \"Designer\":0.9, \"Researcher\":0.6, \"Writer\":0.9, \"Scientist\":0.6, \"Creator\":0.3},\n",
    "    \"Undecided\": {\"Founder\":0.6,\"Engineer\":0.6,\"Designer\":0.6,\"Researcher\":0.6,\"Writer\":0.6,\"Scientist\":0.6,\"Creator\":0.6,\"Undecided\":0.2}\n",
    "}\n",
    "\n",
    "def role_complementarity(df: pd.DataFrame) -> np.ndarray:\n",
    "    roles = df['role'].tolist(); n=len(roles); S=np.zeros((n,n),dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = ROLE_COMP.get(roles[i], {}).get(roles[j], 0.2)\n",
    "    return S\n",
    "\n",
    "def energy_compatibility(energies: List[int], target_gap=0):\n",
    "    e = np.array(energies); n=len(e); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(e[i]-e[j])\n",
    "            S[i,j] = np.exp(-((gap-target_gap)**2)/(2*(1.25**2)))\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "COLLAB_COMP = {\n",
    "    \"async\": {\"async\":1.0, \"hybrid\":0.7, \"sync\":0.3},\n",
    "    \"hybrid\":{\"async\":0.7, \"hybrid\":1.0, \"sync\":0.7},\n",
    "    \"sync\":  {\"async\":0.3, \"hybrid\":0.7, \"sync\":1.0},\n",
    "}\n",
    "def collab_style_compatibility(styles: List[str]) -> np.ndarray:\n",
    "    n=len(styles); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = COLLAB_COMP.get(styles[i],{}).get(styles[j], 0.5)\n",
    "    return S\n",
    "\n",
    "def availability_overlap(avails: List[str]) -> np.ndarray:\n",
    "    map_mid = {\"2-5\":3.5,\"5-10\":7.5,\"10-20\":15.0,\"20+\":25.0}\n",
    "    v = np.array([map_mid.get(a,7.5) for a in avails])\n",
    "    n=len(v); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            gap = abs(v[i]-v[j])\n",
    "            S[i,j] = np.exp(-gap/15.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def time_zone_overlap(tz_list: List[int]) -> np.ndarray:\n",
    "    tz = np.array(tz_list); n=len(tz); S=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            diff = abs(tz[i]-tz[j])\n",
    "            S[i,j] = np.exp(-diff/6.0)\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d24d1",
   "metadata": {},
   "source": [
    "## Skills — similarity & complementarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb1892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_skill_list(sk: str) -> List[str]:\n",
    "    return [s.strip().lower() for s in (sk or \"\").split(\",\") if s.strip()]\n",
    "\n",
    "def tfidf_cosine(a_list: List[str], b_list: List[str]) -> float:\n",
    "    docs = [\"; \".join(a_list), \"; \".join(b_list)]\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "    X = vec.fit_transform(docs)\n",
    "    return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "\n",
    "def similar_skills_matrix(df: pd.DataFrame, embedder: Optional[Embedder] = None) -> np.ndarray:\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    if embedder and getattr(embedder, \"sbert_ok\", False):\n",
    "        corpus = df['skills_have'].fillna('').tolist()\n",
    "        X = embedder.encode(corpus)\n",
    "        S = (X @ X.T)\n",
    "        S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "        np.fill_diagonal(S, 0.0)\n",
    "        return S\n",
    "    parsed = [parse_skill_list(x) for x in df['skills_have'].fillna('')]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S[i,j] = tfidf_cosine(parsed[i], parsed[j])\n",
    "    if S.max()>0: S = S/S.max()\n",
    "    return S\n",
    "\n",
    "def complementary_skills_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    wants = [parse_skill_list(row.get('skills_want', row.get('interests',''))) for _,row in df.iterrows()]\n",
    "    haves = [parse_skill_list(row.get('skills_have','')) for _,row in df.iterrows()]\n",
    "    n=len(df); S=np.zeros((n,n))\n",
    "    try:\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                A = [\"; \".join([n1]) for n1 in need]\n",
    "                B = [\"; \".join([h1]) for h1 in have]\n",
    "                vec = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "                X = vec.fit_transform(A + B)\n",
    "                m, k = len(need), len(have)\n",
    "                Csim = np.zeros((m,k))\n",
    "                for p in range(m):\n",
    "                    for q in range(k):\n",
    "                        Csim[p,q] = cosine_similarity(X[p], X[m+q])[0,0]\n",
    "                size = max(m,k)\n",
    "                padded = np.ones((size,size))\n",
    "                padded[:m,:k] = 1.0 - Csim  # cost = 1 - sim\n",
    "                r_ind, c_ind = linear_sum_assignment(padded)\n",
    "                total_sim = 0.0; count = 0\n",
    "                for r,c in zip(r_ind, c_ind):\n",
    "                    if r < m and c < k:\n",
    "                        total_sim += 1.0 - padded[r,c]; count += 1\n",
    "                S[i,j] = total_sim / (count + 1e-9)\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    except Exception:\n",
    "        for i in range(n):\n",
    "            need = wants[i]\n",
    "            for j in range(n):\n",
    "                if i==j: continue\n",
    "                have = haves[j]\n",
    "                if not need or not have: \n",
    "                    S[i,j]=0.0; continue\n",
    "                sims = []\n",
    "                for nterm in need:\n",
    "                    sims.append(max(tfidf_cosine([nterm], [h]) for h in have))\n",
    "                S[i,j] = float(np.mean(sims)) if sims else 0.0\n",
    "        if S.max()>0: S = S/S.max()\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb895fb",
   "metadata": {},
   "source": [
    "## CF (implicit MF), Graph (PPR) & Fusion (+ reciprocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0600913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mf_als(R: np.ndarray, k: int = 16, alpha: float = 40.0, reg: float = 0.1, iters: int = 6):\n",
    "    # implicit feedback ALS-style (Hu, Koren, Volinsky)\n",
    "    n_users, n_items = R.shape\n",
    "    C = 1 + alpha * R  # confidence\n",
    "    X = np.random.normal(scale=0.1, size=(n_users, k))\n",
    "    Y = np.random.normal(scale=0.1, size=(n_items, k))\n",
    "    eye = np.eye(k)\n",
    "    for _ in range(iters):\n",
    "        for u in range(n_users):\n",
    "            Cu = np.diag(C[u])\n",
    "            YTCuY = Y.T @ Cu @ Y\n",
    "            YTCuPu = Y.T @ (Cu @ R[u])\n",
    "            X[u] = np.linalg.solve(YTCuY + reg*eye, YTCuPu)\n",
    "        for i in range(n_items):\n",
    "            Ci = np.diag(C[:,i])\n",
    "            XTCiX = X.T @ Ci @ X\n",
    "            XTCiPi = X.T @ (Ci @ R[:,i])\n",
    "            Y[i] = np.linalg.solve(XTCiX + reg*eye, XTCiPi)\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    S = Xn @ Xn.T\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-9)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def build_graph_ppr(R: np.ndarray, alpha=0.85) -> np.ndarray:\n",
    "    n = R.shape[0]\n",
    "    G = nx.DiGraph(); G.add_nodes_from(range(n))\n",
    "    edges = [(u,v) for u in range(n) for v in range(n) if R[u,v]>0]\n",
    "    G.add_edges_from(edges)\n",
    "    S = np.zeros((n,n))\n",
    "    for u in range(n):\n",
    "        pr = nx.pagerank(G, alpha=alpha, personalization={k:(1.0 if k==u else 0.0) for k in range(n)})\n",
    "        for v,s in pr.items(): S[u,v] = s\n",
    "    S = (S - S.min())/(S.max()-S.min()+1e-12)\n",
    "    np.fill_diagonal(S, 0.0)\n",
    "    return S\n",
    "\n",
    "def reciprocalize(S: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(S * S.T + 1e-12)\n",
    "\n",
    "def combine_content(S_text, S_geo, S_exp, S_role, S_energy, S_collab, S_avail, S_tz, S_lang, w):\n",
    "    a,b,c,d,e,f,g,h,l = w\n",
    "    S = a*S_text + b*S_geo + c*S_exp + d*S_role + e*S_energy + f*S_collab + g*S_avail + h*S_tz + l*S_lang\n",
    "    return S / (S.max() + 1e-9)\n",
    "\n",
    "def fuse_scores(S_content, S_cf, S_graph, S_person, S_skills, weights=(0.30,0.18,0.16,0.10,0.26)):\n",
    "    Sc = reciprocalize(S_content)\n",
    "    Sf = reciprocalize(S_cf)\n",
    "    Sg = reciprocalize(S_graph)\n",
    "    Sp = reciprocalize(S_person)\n",
    "    Ss = reciprocalize(S_skills)\n",
    "    a,b,c,d,e = weights\n",
    "    S = a*Sc + b*Sf + c*Sg + d*Sp + e*Ss\n",
    "    return S / (S.max() + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504efcc5",
   "metadata": {},
   "source": [
    "## Diversification — MMR / DPP‑style greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5ef65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mmr_rank(query_idx: int, S: np.ndarray, K: int = 5, lambda_rel: float = 0.7):\n",
    "    n = S.shape[0]\n",
    "    candidates = [i for i in range(n) if i != query_idx]\n",
    "    selected = []\n",
    "    while candidates and len(selected) < K:\n",
    "        if not selected:\n",
    "            i = max(candidates, key=lambda j: S[query_idx, j])\n",
    "            selected.append(i); candidates.remove(i)\n",
    "        else:\n",
    "            def score(j):\n",
    "                redundancy = max(S[j, s] for s in selected) if selected else 0.0\n",
    "                return lambda_rel * S[query_idx, j] - (1-lambda_rel) * redundancy\n",
    "            i = max(candidates, key=score)\n",
    "            selected.append(i); candidates.remove(i)\n",
    "    return selected\n",
    "\n",
    "def dpp_greedy(query_idx: int, S: np.ndarray, K: int = 5):\n",
    "    n = S.shape[0]\n",
    "    items = [i for i in range(n) if i != query_idx]\n",
    "    quality = S[query_idx].copy()\n",
    "    q = quality / (quality.max() + 1e-9)\n",
    "    selected = []\n",
    "    remaining = items.copy()\n",
    "    while remaining and len(selected) < K:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax([q[items.index(r)] for r in remaining]))\n",
    "            chosen = remaining[idx]\n",
    "        else:\n",
    "            scores = []\n",
    "            for r in remaining:\n",
    "                max_sim = max(S[r, s] for s in selected) if selected else 0.0\n",
    "                scores.append(q[items.index(r)] - max_sim)\n",
    "            idx = int(np.argmax(scores))\n",
    "            chosen = remaining[idx]\n",
    "        selected.append(chosen); remaining.pop(idx)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6cfa6",
   "metadata": {},
   "source": [
    "## Team formation — greedy submodular objective with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03c6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def team_score(set_ids: List[int], query_idx: int, S_final: np.ndarray, skills_need: List[str], users_df: pd.DataFrame):\n",
    "    if not set_ids: return 0.0\n",
    "    rel = np.mean([S_final[query_idx, j] for j in set_ids])\n",
    "    need = set([s.strip().lower() for s in skills_need if s.strip()])\n",
    "    have = set()\n",
    "    for j in set_ids:\n",
    "        have |= set([s.strip().lower() for s in users_df.iloc[j].skills_have.split(\",\") if s.strip()])\n",
    "    coverage = len(need & have) / (len(need) + 1e-9)\n",
    "    if len(set_ids) > 1:\n",
    "        pair_sims = []\n",
    "        for a in range(len(set_ids)):\n",
    "            for b in range(a+1, len(set_ids)):\n",
    "                pair_sims.append(S_final[set_ids[a], set_ids[b]])\n",
    "        div = 1.0 - float(np.mean(pair_sims))\n",
    "    else:\n",
    "        div = 1.0\n",
    "    return 0.55*rel + 0.30*coverage + 0.15*div\n",
    "\n",
    "def form_team(query_idx: int, S_final: np.ndarray, users_df: pd.DataFrame, K: int, skills_need: List[str],\n",
    "              constraints: Optional[Dict[str, Any]] = None):\n",
    "    n = S_final.shape[0]\n",
    "    candidates = [i for i in range(n) if i != query_idx]\n",
    "    selected = []\n",
    "    def feasible(j):\n",
    "        if not constraints: return True\n",
    "        u = users_df.iloc[query_idx]; v = users_df.iloc[j]\n",
    "        if constraints.get(\"local_radius_km\") is not None:\n",
    "            from geopy.distance import geodesic\n",
    "            d_km = geodesic((u.lat,u.lon), (v.lat,v.lon)).km\n",
    "            if d_km > constraints[\"local_radius_km\"]: return False\n",
    "        if constraints.get(\"require_shared_language\"):\n",
    "            Li = set([x.strip().lower() for x in u.languages.split(\",\") if x.strip()])\n",
    "            Lj = set([x.strip().lower() for x in v.languages.split(\",\") if x.strip()])\n",
    "            if len(Li & Lj)==0: return False\n",
    "        if constraints.get(\"max_tz_diff\") is not None:\n",
    "            if abs(int(u.tz_offset) - int(v.tz_offset)) > constraints[\"max_tz_diff\"]: return False\n",
    "        if constraints.get(\"min_avail_mid\") is not None:\n",
    "            map_mid = {\"2-5\":3.5,\"5-10\":7.5,\"10-20\":15.0,\"20+\":25.0}\n",
    "            if map_mid.get(v.availability_hours, 0) < constraints[\"min_avail_mid\"]: return False\n",
    "        if constraints.get(\"allowed_styles\"):\n",
    "            if v.collab_style not in constraints[\"allowed_styles\"]: return False\n",
    "        return True\n",
    "    while candidates and len(selected) < K:\n",
    "        base = team_score(selected, query_idx, S_final, skills_need, users_df)\n",
    "        best_j, best_gain = None, -1\n",
    "        for j in candidates:\n",
    "            if not feasible(j): \n",
    "                continue\n",
    "            gain = team_score(selected+[j], query_idx, S_final, skills_need, users_df) - base\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_j = gain, j\n",
    "        if best_j is None: break\n",
    "        selected.append(best_j)\n",
    "        candidates.remove(best_j)\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2521151",
   "metadata": {},
   "source": [
    "## Unified matching API — Network expansion, Team build, Field search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4267b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_signals(df: pd.DataFrame, embedder: Embedder):\n",
    "    S_text = build_text_similarity(df, embedder)\n",
    "    S_lang = language_overlap(df)\n",
    "    S_geo  = geo_similarity(df, decay_km=1200.0)\n",
    "    S_exp  = experience_compatibility(df['years_exp'].tolist())\n",
    "    S_role = role_complementarity(df)\n",
    "    S_energy = energy_compatibility(df['energy_1to5'].tolist())\n",
    "    S_collab = collab_style_compatibility(df['collab_style'].tolist())\n",
    "    S_avail  = availability_overlap(df['availability_hours'].tolist())\n",
    "    S_tz     = time_zone_overlap(df['tz_offset'].tolist())\n",
    "    S_sk_sim = similar_skills_matrix(df, embedder if getattr(embedder, \"sbert_ok\", False) else None)\n",
    "    S_sk_comp = complementary_skills_matrix(df)\n",
    "\n",
    "    # Synthetic implicit likes -> CF + graph\n",
    "    n = len(df); R = np.zeros((n,n), dtype=float)\n",
    "    for _ in range(900):\n",
    "        u = random.randrange(n); v = random.randrange(n)\n",
    "        if u==v: continue\n",
    "        if df.iloc[u].role==\"Founder\" and df.iloc[v].role in [\"Engineer\",\"Designer\"]: R[u,v]=1.0\n",
    "        elif df.iloc[u].role==\"Creator\" and df.iloc[v].role in [\"Writer\",\"Designer\",\"Engineer\"]: R[u,v]=1.0\n",
    "        elif df.iloc[u].location_city==df.iloc[v].location_city and random.random()<0.25: R[u,v]=1.0\n",
    "        elif random.random() < 0.04: R[u,v]=1.0\n",
    "    S_cf = mf_als(R, k=16)\n",
    "    S_graph = build_graph_ppr(R, alpha=0.82)\n",
    "\n",
    "    # Personality\n",
    "    S_person = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j: continue\n",
    "            S_person[i,j] = bigfive_cosine(df.iloc[i].bf, df.iloc[j].bf)\n",
    "    S_person = (S_person - S_person.min())/(S_person.max()-S_person.min()+1e-9)\n",
    "\n",
    "    return {\n",
    "        \"S_text\":S_text, \"S_lang\":S_lang, \"S_geo\":S_geo, \"S_exp\":S_exp, \"S_role\":S_role,\n",
    "        \"S_energy\":S_energy, \"S_collab\":S_collab, \"S_avail\":S_avail, \"S_tz\":S_tz,\n",
    "        \"S_sk_sim\":S_sk_sim, \"S_sk_comp\":S_sk_comp, \"S_cf\":S_cf, \"S_graph\":S_graph, \"S_person\":S_person\n",
    "    }\n",
    "\n",
    "def fused_matrix(signals: Dict[str, np.ndarray],\n",
    "                 content_weights=(0.24,0.16,0.10,0.10,0.10,0.10,0.10,0.05,0.05),\n",
    "                 blend_weights=(0.34,0.18,0.14,0.12,0.22),\n",
    "                 skills_mode=\"similar\"):\n",
    "    S_content = combine_content(signals[\"S_text\"], signals[\"S_geo\"], signals[\"S_exp\"], signals[\"S_role\"],\n",
    "                                signals[\"S_energy\"], signals[\"S_collab\"], signals[\"S_avail\"], signals[\"S_tz\"],\n",
    "                                signals[\"S_lang\"], content_weights)\n",
    "    S_sk = signals[\"S_sk_sim\"] if skills_mode==\"similar\" else signals[\"S_sk_comp\"]\n",
    "    S_final = fuse_scores(S_content, signals[\"S_cf\"], signals[\"S_graph\"], signals[\"S_person\"], S_sk, weights=blend_weights)\n",
    "    return S_final, S_content\n",
    "\n",
    "def network_expansion(df: pd.DataFrame, signals: Dict[str,np.ndarray], query_idx: int,\n",
    "                      k:int=10, skills_mode=\"similar\", search_mode=\"local\", local_radius_km=50.0,\n",
    "                      require_shared_language=True, diversifier=\"mmr\"):\n",
    "    S_final, S_content = fused_matrix(signals, skills_mode=skills_mode)\n",
    "    # Local gating\n",
    "    if search_mode == \"local\":\n",
    "        # distance + language hard constraints\n",
    "        coords = list(zip(df.lat, df.lon))\n",
    "        li = (df.iloc[query_idx].lat, df.iloc[query_idx].lon)\n",
    "        mask = np.array([1.0 if i!=query_idx and geodesic(li, coords[i]).km <= local_radius_km else 0.0 for i in range(len(df))])\n",
    "        S_final = S_final * mask.reshape(1,-1)\n",
    "        if require_shared_language:\n",
    "            S_final = S_final * (signals[\"S_lang\"]>0.0)\n",
    "    # Diversify\n",
    "    if diversifier==\"mmr\":\n",
    "        picks = mmr_rank(query_idx, S_final, K=k, lambda_rel=0.72)\n",
    "    elif diversifier==\"dpp\":\n",
    "        picks = dpp_greedy(query_idx, S_final, K=k)\n",
    "    else:\n",
    "        scores = list(enumerate(S_final[query_idx])); scores = [(j,s) for j,s in scores if j!=query_idx]\n",
    "        picks = [j for j,_ in sorted(scores, key=lambda x:-x[1])[:k]]\n",
    "    cols = [\"name\",\"location_city\",\"languages\",\"role\",\"seniority\",\"interests\",\"skills_have\",\"skills_want\",\"years_exp\",\n",
    "            \"age\",\"age_band\",\"energy_1to5\",\"collab_style\",\"availability_hours\",\"reason\"]\n",
    "    out = df.iloc[picks][cols].copy()\n",
    "    out[\"score\"] = [S_final[query_idx,j] for j in picks]\n",
    "    return out\n",
    "\n",
    "def team_build(df: pd.DataFrame, signals: Dict[str,np.ndarray], query_idx: int,\n",
    "               skills_need_text:str, K:int=4, search_mode=\"local\", local_radius_km=50.0,\n",
    "               require_shared_language=True):\n",
    "    S_final, _ = fused_matrix(signals, skills_mode=\"complementary\")  # use complementary for team\n",
    "    need = [s.strip() for s in skills_need_text.split(\",\") if s.strip()]\n",
    "    constraints = {\n",
    "        \"local_radius_km\": local_radius_km if search_mode==\"local\" else None,\n",
    "        \"require_shared_language\": require_shared_language if search_mode==\"local\" else False,\n",
    "        \"max_tz_diff\": 6, \"min_avail_mid\": 5.0, \"allowed_styles\": {\"async\",\"hybrid\",\"sync\"}\n",
    "    }\n",
    "    ids = form_team(query_idx, S_final, df, K=K, skills_need=need, constraints=constraints)\n",
    "    cols = ['name','location_city','languages','role','seniority','skills_have','years_exp','tz_offset','availability_hours','collab_style']\n",
    "    team_df = df.iloc[ids][cols].copy()\n",
    "    team_df['match_score'] = [S_final[query_idx,j] for j in ids]\n",
    "    return team_df\n",
    "\n",
    "def field_search(df: pd.DataFrame, signals: Dict[str,np.ndarray], field_query: str, \n",
    "                 k:int=10, search_mode=\"global\", require_shared_language=False, query_idx: Optional[int]=None):\n",
    "    # Embed the field query vs interests + skills\n",
    "    embedder = Embedder()\n",
    "    texts = (df['interests'].fillna('') + \" ; \" + df['skills_have'].fillna('')).tolist()\n",
    "    embedder.fit(texts + [field_query])\n",
    "    E = embedder.encode(texts + [field_query])\n",
    "    Q = E[-1]; U = E[:-1]\n",
    "    sim = (U @ Q) / (np.linalg.norm(U, axis=1)*np.linalg.norm(Q) + 1e-9)\n",
    "    # Add evidence from skills string match\n",
    "    q_tokens = [t.strip().lower() for t in re.split(r\"[,/;| ]+\", field_query) if t.strip()]\n",
    "    evidence = []\n",
    "    for s in df['skills_have'].fillna(''):\n",
    "        s2 = s.lower()\n",
    "        evidence.append(sum(1 for t in q_tokens if t and t in s2))\n",
    "    evidence = np.array(evidence, dtype=float)\n",
    "    evidence = evidence / (evidence.max() + 1e-9)\n",
    "    # Optionally gate by language/distance relative to a query user\n",
    "    mask = np.ones(len(df), dtype=float)\n",
    "    if search_mode==\"local\" and query_idx is not None:\n",
    "        coords = list(zip(df.lat, df.lon))\n",
    "        li = (df.iloc[query_idx].lat, df.iloc[query_idx].lon)\n",
    "        for i in range(len(df)):\n",
    "            if i==query_idx: \n",
    "                mask[i]=0.0; continue\n",
    "            d_km = geodesic(li, coords[i]).km\n",
    "            if d_km > 60.0: mask[i]=0.0\n",
    "        if require_shared_language:\n",
    "            Lq = set([x.strip().lower() for x in df.iloc[query_idx].languages.split(\",\") if x.strip()])\n",
    "            for i in range(len(df)):\n",
    "                Li = set([x.strip().lower() for x in df.iloc[i].languages.split(\",\") if x.strip()])\n",
    "                if len(Lq & Li)==0: mask[i]=0.0\n",
    "    # Score: 0.65 semantic + 0.35 evidence, then mask and rank\n",
    "    score = 0.65*sim + 0.35*evidence\n",
    "    score = score * mask\n",
    "    order = np.argsort(-score)\n",
    "    picks = [i for i in order if score[i]>0][:k]\n",
    "    cols = [\"name\",\"location_city\",\"languages\",\"role\",\"seniority\",\"interests\",\"skills_have\",\"years_exp\"]\n",
    "    out = df.iloc[picks][cols].copy()\n",
    "    out[\"field_score\"] = [float(score[i]) for i in picks]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8033f",
   "metadata": {},
   "source": [
    "## Synthetic cohort — 150 users across cities & languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb30ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_band</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_country</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>years_exp</th>\n",
       "      <th>skills_have</th>\n",
       "      <th>skills_want</th>\n",
       "      <th>interests</th>\n",
       "      <th>human</th>\n",
       "      <th>professional</th>\n",
       "      <th>contributor</th>\n",
       "      <th>interests_long</th>\n",
       "      <th>reason</th>\n",
       "      <th>bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User001</td>\n",
       "      <td>1968-03-23</td>\n",
       "      <td>57</td>\n",
       "      <td>55+</td>\n",
       "      <td>Sydney</td>\n",
       "      <td></td>\n",
       "      <td>-33.8688</td>\n",
       "      <td>151.2093</td>\n",
       "      <td>10</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>product, scriptwriting, design systems, grant ...</td>\n",
       "      <td>causal inference, genomics</td>\n",
       "      <td>sports analytics, VR social spaces, music prod...</td>\n",
       "      <td>I live in Sydney. I like calm schedules and me...</td>\n",
       "      <td>As a engineer with 1 years, I worked across st...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: build ocean microplastics sensors.</td>\n",
       "      <td>Expand network</td>\n",
       "      <td>BigFive(O=0.5, C=0.3333333333333333, E=0.83333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User002</td>\n",
       "      <td>1971-10-06</td>\n",
       "      <td>53</td>\n",
       "      <td>45-54</td>\n",
       "      <td>London</td>\n",
       "      <td></td>\n",
       "      <td>51.5072</td>\n",
       "      <td>-0.1276</td>\n",
       "      <td>0</td>\n",
       "      <td>fr, de</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>nextjs, prompt engineering, design systems, ge...</td>\n",
       "      <td>marketing, django, design systems, podcasting</td>\n",
       "      <td>long-form YouTube, mental health, newsletter g...</td>\n",
       "      <td>I live in London. I like calm schedules and me...</td>\n",
       "      <td>As a researcher with 9 years, I worked across ...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.3333333333333333, C=0.5, E=0.75, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User003</td>\n",
       "      <td>1962-12-18</td>\n",
       "      <td>62</td>\n",
       "      <td>55+</td>\n",
       "      <td>New York</td>\n",
       "      <td></td>\n",
       "      <td>40.7128</td>\n",
       "      <td>-74.0060</td>\n",
       "      <td>-5</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>sql, causal inference, strategy, python, lab t...</td>\n",
       "      <td>pytorch, storyboarding, strategy, seo, biostat...</td>\n",
       "      <td>publishing, beauty brand, climate tech, financ...</td>\n",
       "      <td>I live in New York. I like calm schedules and ...</td>\n",
       "      <td>As a founder with 2 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: build ocean microplastics sensors.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.75, C=0.24999999999999997, E=0.416...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name         dob  age age_band location_city location_country      lat  \\\n",
       "0  User001  1968-03-23   57      55+        Sydney                  -33.8688   \n",
       "1  User002  1971-10-06   53    45-54        London                   51.5072   \n",
       "2  User003  1962-12-18   62      55+      New York                   40.7128   \n",
       "\n",
       "        lon  tz_offset languages  ... years_exp  \\\n",
       "0  151.2093         10        en  ...         1   \n",
       "1   -0.1276          0    fr, de  ...         9   \n",
       "2  -74.0060         -5        en  ...         2   \n",
       "\n",
       "                                         skills_have  \\\n",
       "0  product, scriptwriting, design systems, grant ...   \n",
       "1  nextjs, prompt engineering, design systems, ge...   \n",
       "2  sql, causal inference, strategy, python, lab t...   \n",
       "\n",
       "                                         skills_want  \\\n",
       "0                         causal inference, genomics   \n",
       "1      marketing, django, design systems, podcasting   \n",
       "2  pytorch, storyboarding, strategy, seo, biostat...   \n",
       "\n",
       "                                           interests  \\\n",
       "0  sports analytics, VR social spaces, music prod...   \n",
       "1  long-form YouTube, mental health, newsletter g...   \n",
       "2  publishing, beauty brand, climate tech, financ...   \n",
       "\n",
       "                                               human  \\\n",
       "0  I live in Sydney. I like calm schedules and me...   \n",
       "1  I live in London. I like calm schedules and me...   \n",
       "2  I live in New York. I like calm schedules and ...   \n",
       "\n",
       "                                        professional  \\\n",
       "0  As a engineer with 1 years, I worked across st...   \n",
       "1  As a researcher with 9 years, I worked across ...   \n",
       "2  As a founder with 2 years, I worked across sta...   \n",
       "\n",
       "                                         contributor  \\\n",
       "0  I prefer weekly demos and short design docs. I...   \n",
       "1  I prefer weekly demos and short design docs. I...   \n",
       "2  I prefer weekly demos and short design docs. I...   \n",
       "\n",
       "                                 interests_long          reason  \\\n",
       "0     Goals: build ocean microplastics sensors.  Expand network   \n",
       "1  Goals: prototype a privacy-first social app.   Find projects   \n",
       "2     Goals: build ocean microplastics sensors.   Find projects   \n",
       "\n",
       "                                                  bf  \n",
       "0  BigFive(O=0.5, C=0.3333333333333333, E=0.83333...  \n",
       "1  BigFive(O=0.3333333333333333, C=0.5, E=0.75, A...  \n",
       "2  BigFive(O=0.75, C=0.24999999999999997, E=0.416...  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "roles = [\"Founder\",\"Engineer\",\"Designer\",\"Researcher\",\"Writer\",\"Scientist\",\"Creator\"]\n",
    "seniorities = [\"Junior\",\"Mid\",\"Senior\",\"Lead/Principal\",\"Executive/Founder\"]\n",
    "cities = [\n",
    "    (\"Toronto\",43.6532,-79.3832,-5, [\"en\",\"fr\"]),\n",
    "    (\"New York\",40.7128,-74.0060,-5, [\"en\",\"es\"]),\n",
    "    (\"San Francisco\",37.7749,-122.4194,-8, [\"en\",\"zh\"]),\n",
    "    (\"London\",51.5072,-0.1276,0, [\"en\",\"fr\",\"de\"]),\n",
    "    (\"Berlin\",52.52,13.405,1, [\"de\",\"en\"]),\n",
    "    (\"Nairobi\",-1.286389,36.817223,3, [\"en\",\"sw\"]),\n",
    "    (\"Sydney\",-33.8688,151.2093,10, [\"en\"]),\n",
    "    (\"Bangalore\",12.9716,77.5946,5, [\"en\",\"hi\"]),\n",
    "    (\"Paris\",48.8566,2.3522,1, [\"fr\",\"en\"]),\n",
    "    (\"Mexico City\",19.4326,-99.1332,-6, [\"es\",\"en\"])\n",
    "]\n",
    "skill_bank = list(SKILL_LEXICON)\n",
    "interest_bank = [\n",
    "    \"ocean conservation\",\"coral reef restoration\",\"climate tech\",\"educational apps\",\"healthcare AI\",\n",
    "    \"creator economy\",\"open source tools\",\"social impact\",\"rural connectivity\",\"financial inclusion\",\n",
    "    \"short-form video\",\"long-form YouTube\",\"beauty brand\",\"lipstick R&D\",\"fashion sustainability\",\n",
    "    \"music production\",\"publishing\",\"newsletter growth\",\"sports analytics\",\"mental health\",\n",
    "    \"language learning\",\"VR social spaces\",\"next social network\",\"privacy-first messaging\"\n",
    "]\n",
    "\n",
    "def rand_words(pool, kmin, kmax):\n",
    "    k = random.randint(kmin, kmax)\n",
    "    return \", \".join(random.sample(pool, k))\n",
    "\n",
    "def random_dob():\n",
    "    y = random.randint(1961, 2004)  # ensure 21+\n",
    "    m = random.randint(1,12); d = random.randint(1,28)\n",
    "    return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "\n",
    "def mk_user(i):\n",
    "    name = f\"User{i:03d}\"\n",
    "    (city, lat, lon, tz, langs_base) = random.choice(cities)\n",
    "    role = random.choice(roles)\n",
    "    seniority = random.choice(seniorities)\n",
    "    skills_have = rand_words(skill_bank, 3, 7)\n",
    "    skills_want = rand_words(skill_bank, 2, 5)\n",
    "    interests = rand_words(interest_bank, 3, 7)\n",
    "    years = random.randint(1, 18)\n",
    "    human = f\"I live in {city}. I like calm schedules and meetups; I enjoy running and cooking.\"\n",
    "    professional = f\"As a {role.lower()} with {years} years, I worked across startups and labs. I can produce prototypes, brand systems, docs, and production code.\"\n",
    "    contributor = \"I prefer weekly demos and short design docs. I bring reliability, curiosity, and momentum to small teams with clear ownership.\"\n",
    "    interests_long = f\"Goals: {random.choice(['launch a YouTube channel on ML','build ocean microplastics sensors','start a cruelty-free lipstick brand','prototype a privacy-first social app'])}.\"\n",
    "    reason = random.choice([\"Find projects\",\"Expand network\",\"Find collaborators\",\"Build a dream\"])\n",
    "    langs = random.sample(langs_base, min(len(langs_base), random.choice([1,1,2])))\n",
    "    row = dict(\n",
    "        name=name, dob=random_dob(), location_city=city, location_country=\"\",\n",
    "        lat=lat, lon=lon, tz_offset=tz, languages=\", \".join(langs),\n",
    "        availability_hours=random.choice([\"2-5\",\"5-10\",\"10-20\",\"20+\"]),\n",
    "        energy_1to5=random.randint(1,5), collab_style=random.choice([\"async\",\"hybrid\",\"sync\"]),\n",
    "        role=role, seniority=seniority, years_exp=years,\n",
    "        skills_have=skills_have, skills_want=skills_want, interests=interests,\n",
    "        human=human, professional=professional, contributor=contributor, interests_long=interests_long, reason=reason\n",
    "    )\n",
    "    return normalize_intake(row)\n",
    "\n",
    "records = [mk_user(i) for i in range(1,151)]\n",
    "tipi_all = [[random.randint(2,6) for _ in range(10)] for __ in range(150)]\n",
    "bfs = [score_tipi(t) for t in tipi_all]\n",
    "users = pd.DataFrame(records)\n",
    "users['bf'] = bfs\n",
    "users.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77538bcb",
   "metadata": {},
   "source": [
    "## Build similarity signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc87484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camerondeardon/Documents/Development/Projects/Engineering/rocket/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedder = Embedder()\n",
    "signals = build_signals(users, embedder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab33e4",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e869b0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Network expansion (LOCAL Toronto) ===\n",
      "   name location_city languages       role    score\n",
      "User106       Toronto        fr Researcher 0.834267\n",
      "User134       Toronto        fr    Founder 0.822780\n",
      "User012       Toronto        fr     Writer 0.716035\n",
      "User001        Sydney        en   Engineer 0.000000\n",
      "User003      New York        en    Founder 0.000000\n",
      "User004   Mexico City        es   Engineer 0.000000\n",
      "User005       Nairobi        sw    Creator 0.000000\n",
      "User006   Mexico City        en Researcher 0.000000\n",
      "\n",
      "=== Network expansion (GLOBAL) ===\n",
      "   name location_city languages       role    score\n",
      "User058   Mexico City    en, es   Engineer 0.532085\n",
      "User042         Paris        en Researcher 0.622797\n",
      "User135       Nairobi        sw    Creator 0.605734\n",
      "User107     Bangalore        en Researcher 0.649144\n",
      "User017        Berlin        de    Founder 0.703539\n",
      "User128        Sydney        en   Designer 0.631466\n",
      "User051 San Francisco    zh, en   Designer 0.490502\n",
      "User114         Paris        fr Researcher 0.702731\n",
      "\n",
      "=== Local team (NYC) for: react, product, branding, growth ===\n",
      "   name location_city languages      role         seniority                                                       skills_have  years_exp  tz_offset availability_hours collab_style  match_score\n",
      "User024      New York    es, en Scientist            Senior                                              growth, dbt, pytorch         18         -5               5-10       hybrid     0.579345\n",
      "User082      New York    es, en    Writer Executive/Founder statistics, nextjs, causal inference, sql, tensorflow, podcasting          3         -5                20+       hybrid     0.643129\n",
      "User035      New York        en Scientist            Junior    kubernetes, aws, airflow, figma, dbt, lab techniques, strategy         16         -5                20+         sync     0.649571\n",
      "User064      New York    en, es  Engineer    Lead/Principal          field research, scriptwriting, pytorch, causal inference         12         -5               5-10         sync     0.645556\n",
      "\n",
      "=== Field search (GLOBAL): ocean conservation ===\n",
      "   name location_city languages     role                                                                       skills_have  years_exp  field_score\n",
      "User094 San Francisco        zh  Creator                                          cv, oceanography, gcp, pytorch, strategy          8     0.754090\n",
      "User104     Bangalore        en  Creator seo, video editing, field research, kubernetes, dbt, oceanography, lab techniques         17     0.721161\n",
      "User147     Bangalore        hi Designer             oceanography, python, lab techniques, nlp, fundraising, grant writing          2     0.703877\n",
      "User049 San Francisco        zh Engineer                           podcasting, oceanography, field research, aws, genomics          2     0.660870\n",
      "User052   Mexico City        en Designer           branding, podcasting, oceanography, kubernetes, prompt engineering, nlp          6     0.651834\n",
      "User018       Nairobi    sw, en   Writer                                                  growth, oceanography, figma, aws          2     0.646305\n",
      "User068       Nairobi        sw  Founder            marketing, biostatistics, growth, oceanography, cv, prompt engineering          3     0.629271\n",
      "User010        Berlin        en   Writer  genomics, cv, supply chain, dbt, grant writing, oceanography, prompt engineering         15     0.613783\n",
      "\n",
      "=== Field search (LOCAL SF): next social network ===\n",
      "   name location_city languages       role                                                                             skills_have  years_exp  field_score\n",
      "User115 San Francisco    zh, en  Scientist                                      strategy, django, tensorflow, nextjs, go, branding          4     0.523736\n",
      "User050 San Francisco    en, zh  Scientist                                    lab techniques, gcp, aws, django, design systems, go         11     0.283249\n",
      "User099 San Francisco    zh, en    Creator                                      biostatistics, fundraising, strategy, supply chain         15     0.240360\n",
      "User040 San Francisco    zh, en Researcher                                        react, go, scriptwriting, kubernetes, statistics          7     0.232704\n",
      "User094 San Francisco        zh    Creator                                                cv, oceanography, gcp, pytorch, strategy          8     0.229648\n",
      "User022 San Francisco        zh    Founder prompt engineering, airflow, podcasting, lab techniques, python, scriptwriting, product         16     0.215752\n",
      "User149 San Francisco    zh, en    Creator                                                             podcasting, tensorflow, gcp          6     0.197248\n",
      "User130 San Francisco    zh, en  Scientist                                                                      strategy, aws, gcp         14     0.194648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Network expansion — local (Toronto) & global\n",
    "tor_idx = users.index[users.location_city==\"Toronto\"][0]\n",
    "local_net = network_expansion(users, signals, tor_idx, k=8, skills_mode=\"similar\", search_mode=\"local\", local_radius_km=60.0, require_shared_language=True, diversifier=\"mmr\")\n",
    "global_net = network_expansion(users, signals, tor_idx, k=8, skills_mode=\"similar\", search_mode=\"global\", diversifier=\"dpp\")\n",
    "print(\"=== Network expansion (LOCAL Toronto) ===\")\n",
    "print(local_net[['name','location_city','languages','role','score']].to_string(index=False))\n",
    "print(\"\\n=== Network expansion (GLOBAL) ===\")\n",
    "print(global_net[['name','location_city','languages','role','score']].to_string(index=False))\n",
    "\n",
    "# 2) Team build — local NYC\n",
    "ny_idx = users.index[users.location_city==\"New York\"][0]\n",
    "team = team_build(users, signals, ny_idx, skills_need_text=\"react, product, branding, growth\", K=4, search_mode=\"local\", local_radius_km=50.0, require_shared_language=True)\n",
    "print(\"\\n=== Local team (NYC) for: react, product, branding, growth ===\")\n",
    "print(team.to_string(index=False))\n",
    "\n",
    "# 3) Field search — ocean conservation (global) + next social network (local to SF if available)\n",
    "field_global = field_search(users, signals, \"ocean conservation\", k=8, search_mode=\"global\")\n",
    "print(\"\\n=== Field search (GLOBAL): ocean conservation ===\")\n",
    "print(field_global[['name','location_city','languages','role','skills_have','years_exp','field_score']].to_string(index=False))\n",
    "\n",
    "sf = users.index[users.location_city==\"San Francisco\"][0]\n",
    "field_local = field_search(users, signals, \"next social network\", k=8, search_mode=\"local\", require_shared_language=True, query_idx=sf)\n",
    "print(\"\\n=== Field search (LOCAL SF): next social network ===\")\n",
    "print(field_local[['name','location_city','languages','role','skills_have','years_exp','field_score']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd37fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_band</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_country</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tz_offset</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>years_exp</th>\n",
       "      <th>skills_have</th>\n",
       "      <th>skills_want</th>\n",
       "      <th>interests</th>\n",
       "      <th>human</th>\n",
       "      <th>professional</th>\n",
       "      <th>contributor</th>\n",
       "      <th>interests_long</th>\n",
       "      <th>reason</th>\n",
       "      <th>bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User001</td>\n",
       "      <td>1968-03-23</td>\n",
       "      <td>57</td>\n",
       "      <td>55+</td>\n",
       "      <td>Sydney</td>\n",
       "      <td></td>\n",
       "      <td>-33.868800</td>\n",
       "      <td>151.209300</td>\n",
       "      <td>10</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>product, scriptwriting, design systems, grant ...</td>\n",
       "      <td>causal inference, genomics</td>\n",
       "      <td>sports analytics, VR social spaces, music prod...</td>\n",
       "      <td>I live in Sydney. I like calm schedules and me...</td>\n",
       "      <td>As a engineer with 1 years, I worked across st...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: build ocean microplastics sensors.</td>\n",
       "      <td>Expand network</td>\n",
       "      <td>BigFive(O=0.5, C=0.3333333333333333, E=0.83333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User002</td>\n",
       "      <td>1971-10-06</td>\n",
       "      <td>53</td>\n",
       "      <td>45-54</td>\n",
       "      <td>London</td>\n",
       "      <td></td>\n",
       "      <td>51.507200</td>\n",
       "      <td>-0.127600</td>\n",
       "      <td>0</td>\n",
       "      <td>fr, de</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>nextjs, prompt engineering, design systems, ge...</td>\n",
       "      <td>marketing, django, design systems, podcasting</td>\n",
       "      <td>long-form YouTube, mental health, newsletter g...</td>\n",
       "      <td>I live in London. I like calm schedules and me...</td>\n",
       "      <td>As a researcher with 9 years, I worked across ...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.3333333333333333, C=0.5, E=0.75, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User003</td>\n",
       "      <td>1962-12-18</td>\n",
       "      <td>62</td>\n",
       "      <td>55+</td>\n",
       "      <td>New York</td>\n",
       "      <td></td>\n",
       "      <td>40.712800</td>\n",
       "      <td>-74.006000</td>\n",
       "      <td>-5</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>sql, causal inference, strategy, python, lab t...</td>\n",
       "      <td>pytorch, storyboarding, strategy, seo, biostat...</td>\n",
       "      <td>publishing, beauty brand, climate tech, financ...</td>\n",
       "      <td>I live in New York. I like calm schedules and ...</td>\n",
       "      <td>As a founder with 2 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: build ocean microplastics sensors.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.75, C=0.24999999999999997, E=0.416...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User004</td>\n",
       "      <td>1993-08-05</td>\n",
       "      <td>32</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td></td>\n",
       "      <td>19.432600</td>\n",
       "      <td>-99.133200</td>\n",
       "      <td>-6</td>\n",
       "      <td>es</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>python, airflow, react, marketing, seo, genomics</td>\n",
       "      <td>statistics, prompt engineering, product, suppl...</td>\n",
       "      <td>social impact, fashion sustainability, long-fo...</td>\n",
       "      <td>I live in Mexico City. I like calm schedules a...</td>\n",
       "      <td>As a engineer with 5 years, I worked across st...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: launch a YouTube channel on ML.</td>\n",
       "      <td>Find projects</td>\n",
       "      <td>BigFive(O=0.41666666666666663, C=0.16666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User005</td>\n",
       "      <td>1987-04-23</td>\n",
       "      <td>38</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td></td>\n",
       "      <td>-1.286389</td>\n",
       "      <td>36.817223</td>\n",
       "      <td>3</td>\n",
       "      <td>sw</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>figma, airflow, sql, aws, oceanography</td>\n",
       "      <td>seo, marketing, figma, dbt</td>\n",
       "      <td>healthcare AI, privacy-first messaging, VR soc...</td>\n",
       "      <td>I live in Nairobi. I like calm schedules and m...</td>\n",
       "      <td>As a creator with 4 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: launch a YouTube channel on ML.</td>\n",
       "      <td>Build a dream</td>\n",
       "      <td>BigFive(O=0.5833333333333334, C=0.416666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>User146</td>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>35</td>\n",
       "      <td>35-44</td>\n",
       "      <td>London</td>\n",
       "      <td></td>\n",
       "      <td>51.507200</td>\n",
       "      <td>-0.127600</td>\n",
       "      <td>0</td>\n",
       "      <td>fr, de</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>product, figma, branding, scriptwriting, nlp, ...</td>\n",
       "      <td>kubernetes, supply chain</td>\n",
       "      <td>lipstick R&amp;D, long-form YouTube, short-form vi...</td>\n",
       "      <td>I live in London. I like calm schedules and me...</td>\n",
       "      <td>As a engineer with 15 years, I worked across s...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: build ocean microplastics sensors.</td>\n",
       "      <td>Build a dream</td>\n",
       "      <td>BigFive(O=0.5, C=0.5833333333333334, E=0.75, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>User147</td>\n",
       "      <td>1968-05-05</td>\n",
       "      <td>57</td>\n",
       "      <td>55+</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td></td>\n",
       "      <td>12.971600</td>\n",
       "      <td>77.594600</td>\n",
       "      <td>5</td>\n",
       "      <td>hi</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>oceanography, python, lab techniques, nlp, fun...</td>\n",
       "      <td>tensorflow, figma</td>\n",
       "      <td>rural connectivity, ocean conservation, creato...</td>\n",
       "      <td>I live in Bangalore. I like calm schedules and...</td>\n",
       "      <td>As a designer with 2 years, I worked across st...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: launch a YouTube channel on ML.</td>\n",
       "      <td>Build a dream</td>\n",
       "      <td>BigFive(O=0.16666666666666663, C=0.41666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>User148</td>\n",
       "      <td>1978-06-25</td>\n",
       "      <td>47</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td></td>\n",
       "      <td>19.432600</td>\n",
       "      <td>-99.133200</td>\n",
       "      <td>-6</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>biostatistics, airflow, causal inference, fiel...</td>\n",
       "      <td>grant writing, aws, nlp, figma</td>\n",
       "      <td>publishing, educational apps, ocean conservati...</td>\n",
       "      <td>I live in Mexico City. I like calm schedules a...</td>\n",
       "      <td>As a researcher with 13 years, I worked across...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Find collaborators</td>\n",
       "      <td>BigFive(O=0.33333333333333337, C=0.5, E=0.4166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>User149</td>\n",
       "      <td>1968-04-04</td>\n",
       "      <td>57</td>\n",
       "      <td>55+</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td></td>\n",
       "      <td>37.774900</td>\n",
       "      <td>-122.419400</td>\n",
       "      <td>-8</td>\n",
       "      <td>zh, en</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>podcasting, tensorflow, gcp</td>\n",
       "      <td>react, prompt engineering</td>\n",
       "      <td>open source tools, social impact, coral reef r...</td>\n",
       "      <td>I live in San Francisco. I like calm schedules...</td>\n",
       "      <td>As a creator with 6 years, I worked across sta...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: prototype a privacy-first social app.</td>\n",
       "      <td>Build a dream</td>\n",
       "      <td>BigFive(O=0.3333333333333333, C=0.666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>User150</td>\n",
       "      <td>1988-05-15</td>\n",
       "      <td>37</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td></td>\n",
       "      <td>-1.286389</td>\n",
       "      <td>36.817223</td>\n",
       "      <td>3</td>\n",
       "      <td>sw</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>aws, pytorch, lab techniques</td>\n",
       "      <td>lab techniques, pytorch, product, airflow</td>\n",
       "      <td>sports analytics, music production, beauty bra...</td>\n",
       "      <td>I live in Nairobi. I like calm schedules and m...</td>\n",
       "      <td>As a creator with 18 years, I worked across st...</td>\n",
       "      <td>I prefer weekly demos and short design docs. I...</td>\n",
       "      <td>Goals: launch a YouTube channel on ML.</td>\n",
       "      <td>Find collaborators</td>\n",
       "      <td>BigFive(O=0.16666666666666663, C=0.75, E=0.666...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        name         dob  age age_band  location_city location_country  \\\n",
       "0    User001  1968-03-23   57      55+         Sydney                    \n",
       "1    User002  1971-10-06   53    45-54         London                    \n",
       "2    User003  1962-12-18   62      55+       New York                    \n",
       "3    User004  1993-08-05   32    25-34    Mexico City                    \n",
       "4    User005  1987-04-23   38    35-44        Nairobi                    \n",
       "..       ...         ...  ...      ...            ...              ...   \n",
       "145  User146  1990-01-05   35    35-44         London                    \n",
       "146  User147  1968-05-05   57      55+      Bangalore                    \n",
       "147  User148  1978-06-25   47    45-54    Mexico City                    \n",
       "148  User149  1968-04-04   57      55+  San Francisco                    \n",
       "149  User150  1988-05-15   37    35-44        Nairobi                    \n",
       "\n",
       "           lat         lon  tz_offset languages  ... years_exp  \\\n",
       "0   -33.868800  151.209300         10        en  ...         1   \n",
       "1    51.507200   -0.127600          0    fr, de  ...         9   \n",
       "2    40.712800  -74.006000         -5        en  ...         2   \n",
       "3    19.432600  -99.133200         -6        es  ...         5   \n",
       "4    -1.286389   36.817223          3        sw  ...         4   \n",
       "..         ...         ...        ...       ...  ...       ...   \n",
       "145  51.507200   -0.127600          0    fr, de  ...        15   \n",
       "146  12.971600   77.594600          5        hi  ...         2   \n",
       "147  19.432600  -99.133200         -6        en  ...        13   \n",
       "148  37.774900 -122.419400         -8    zh, en  ...         6   \n",
       "149  -1.286389   36.817223          3        sw  ...        18   \n",
       "\n",
       "                                           skills_have  \\\n",
       "0    product, scriptwriting, design systems, grant ...   \n",
       "1    nextjs, prompt engineering, design systems, ge...   \n",
       "2    sql, causal inference, strategy, python, lab t...   \n",
       "3     python, airflow, react, marketing, seo, genomics   \n",
       "4               figma, airflow, sql, aws, oceanography   \n",
       "..                                                 ...   \n",
       "145  product, figma, branding, scriptwriting, nlp, ...   \n",
       "146  oceanography, python, lab techniques, nlp, fun...   \n",
       "147  biostatistics, airflow, causal inference, fiel...   \n",
       "148                        podcasting, tensorflow, gcp   \n",
       "149                       aws, pytorch, lab techniques   \n",
       "\n",
       "                                           skills_want  \\\n",
       "0                           causal inference, genomics   \n",
       "1        marketing, django, design systems, podcasting   \n",
       "2    pytorch, storyboarding, strategy, seo, biostat...   \n",
       "3    statistics, prompt engineering, product, suppl...   \n",
       "4                           seo, marketing, figma, dbt   \n",
       "..                                                 ...   \n",
       "145                           kubernetes, supply chain   \n",
       "146                                  tensorflow, figma   \n",
       "147                     grant writing, aws, nlp, figma   \n",
       "148                          react, prompt engineering   \n",
       "149          lab techniques, pytorch, product, airflow   \n",
       "\n",
       "                                             interests  \\\n",
       "0    sports analytics, VR social spaces, music prod...   \n",
       "1    long-form YouTube, mental health, newsletter g...   \n",
       "2    publishing, beauty brand, climate tech, financ...   \n",
       "3    social impact, fashion sustainability, long-fo...   \n",
       "4    healthcare AI, privacy-first messaging, VR soc...   \n",
       "..                                                 ...   \n",
       "145  lipstick R&D, long-form YouTube, short-form vi...   \n",
       "146  rural connectivity, ocean conservation, creato...   \n",
       "147  publishing, educational apps, ocean conservati...   \n",
       "148  open source tools, social impact, coral reef r...   \n",
       "149  sports analytics, music production, beauty bra...   \n",
       "\n",
       "                                                 human  \\\n",
       "0    I live in Sydney. I like calm schedules and me...   \n",
       "1    I live in London. I like calm schedules and me...   \n",
       "2    I live in New York. I like calm schedules and ...   \n",
       "3    I live in Mexico City. I like calm schedules a...   \n",
       "4    I live in Nairobi. I like calm schedules and m...   \n",
       "..                                                 ...   \n",
       "145  I live in London. I like calm schedules and me...   \n",
       "146  I live in Bangalore. I like calm schedules and...   \n",
       "147  I live in Mexico City. I like calm schedules a...   \n",
       "148  I live in San Francisco. I like calm schedules...   \n",
       "149  I live in Nairobi. I like calm schedules and m...   \n",
       "\n",
       "                                          professional  \\\n",
       "0    As a engineer with 1 years, I worked across st...   \n",
       "1    As a researcher with 9 years, I worked across ...   \n",
       "2    As a founder with 2 years, I worked across sta...   \n",
       "3    As a engineer with 5 years, I worked across st...   \n",
       "4    As a creator with 4 years, I worked across sta...   \n",
       "..                                                 ...   \n",
       "145  As a engineer with 15 years, I worked across s...   \n",
       "146  As a designer with 2 years, I worked across st...   \n",
       "147  As a researcher with 13 years, I worked across...   \n",
       "148  As a creator with 6 years, I worked across sta...   \n",
       "149  As a creator with 18 years, I worked across st...   \n",
       "\n",
       "                                           contributor  \\\n",
       "0    I prefer weekly demos and short design docs. I...   \n",
       "1    I prefer weekly demos and short design docs. I...   \n",
       "2    I prefer weekly demos and short design docs. I...   \n",
       "3    I prefer weekly demos and short design docs. I...   \n",
       "4    I prefer weekly demos and short design docs. I...   \n",
       "..                                                 ...   \n",
       "145  I prefer weekly demos and short design docs. I...   \n",
       "146  I prefer weekly demos and short design docs. I...   \n",
       "147  I prefer weekly demos and short design docs. I...   \n",
       "148  I prefer weekly demos and short design docs. I...   \n",
       "149  I prefer weekly demos and short design docs. I...   \n",
       "\n",
       "                                   interests_long              reason  \\\n",
       "0       Goals: build ocean microplastics sensors.      Expand network   \n",
       "1    Goals: prototype a privacy-first social app.       Find projects   \n",
       "2       Goals: build ocean microplastics sensors.       Find projects   \n",
       "3          Goals: launch a YouTube channel on ML.       Find projects   \n",
       "4          Goals: launch a YouTube channel on ML.       Build a dream   \n",
       "..                                            ...                 ...   \n",
       "145     Goals: build ocean microplastics sensors.       Build a dream   \n",
       "146        Goals: launch a YouTube channel on ML.       Build a dream   \n",
       "147  Goals: prototype a privacy-first social app.  Find collaborators   \n",
       "148  Goals: prototype a privacy-first social app.       Build a dream   \n",
       "149        Goals: launch a YouTube channel on ML.  Find collaborators   \n",
       "\n",
       "                                                    bf  \n",
       "0    BigFive(O=0.5, C=0.3333333333333333, E=0.83333...  \n",
       "1    BigFive(O=0.3333333333333333, C=0.5, E=0.75, A...  \n",
       "2    BigFive(O=0.75, C=0.24999999999999997, E=0.416...  \n",
       "3    BigFive(O=0.41666666666666663, C=0.16666666666...  \n",
       "4    BigFive(O=0.5833333333333334, C=0.416666666666...  \n",
       "..                                                 ...  \n",
       "145  BigFive(O=0.5, C=0.5833333333333334, E=0.75, A...  \n",
       "146  BigFive(O=0.16666666666666663, C=0.41666666666...  \n",
       "147  BigFive(O=0.33333333333333337, C=0.5, E=0.4166...  \n",
       "148  BigFive(O=0.3333333333333333, C=0.666666666666...  \n",
       "149  BigFive(O=0.16666666666666663, C=0.75, E=0.666...  \n",
       "\n",
       "[150 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbec5ca",
   "metadata": {},
   "source": [
    "## Final — print ALL names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_names_df = users[['name']].copy()\n",
    "print(all_names_df.to_string(index=False))\n",
    "all_names_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
